{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_lms.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/lverwimp/RNN_language_modeling/blob/master/rnn_lms_extended.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "esuUK2ev_Nyt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ]
    },
    {
      "metadata": {
        "id": "_yKJ-Plo_HTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib, collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UScGV-tTaZba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some global variables:"
      ]
    },
    {
      "metadata": {
        "id": "KNbwMREbacvY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 20\n",
        "NUM_STEPS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hmCKPx4KEcG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reading the data"
      ]
    },
    {
      "metadata": {
        "id": "-oKtBwJQAJED",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get training, validation and test data:"
      ]
    },
    {
      "metadata": {
        "id": "t6AIeiR2BaMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_url = 'http://homes.esat.kuleuven.be/~lverwimp/course_speech_recognition/train.txt'\n",
        "valid_url = 'http://homes.esat.kuleuven.be/~lverwimp/course_speech_recognition/valid.txt'\n",
        "test_url = 'http://homes.esat.kuleuven.be/~lverwimp/course_speech_recognition/test.txt'\n",
        "train_file = urllib.urlopen(train_url).read()\n",
        "valid_file = urllib.urlopen(valid_url).read()\n",
        "test_file = urllib.urlopen(test_url).read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Ye3oJavH-8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "GXEcoaWKIAtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "25354e27-c157-41db-9603-80d8e2740491"
      },
      "cell_type": "code",
      "source": [
        "print('{0}...'.format(valid_file[:500]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " consumers may want to move their telephones a little closer to the tv set \n",
            " <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            " two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues \n",
            " and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day 's show \n",
            " interactive telephone technology...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ZF1PoUJDRkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<unk\\> is a symbol for the unknown words class, 'N' is a symbol used for the numbers class.\n",
        "  \n",
        "Convert data to correct format:"
      ]
    },
    {
      "metadata": {
        "id": "Y531EV5aDVd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert the string to a list and replace newlines with the end-of-sentence symbol\n",
        "train_text = [w for w in train_file.replace('\\n',' <eos>').split(' ')]\n",
        "valid_text = [w for w in valid_file.replace('\\n',' <eos>').split(' ')]\n",
        "test_text = [w for w in test_file.replace('\\n',' <eos>').split(' ')]\n",
        "\n",
        "# count the frequencies of the words in the training data\n",
        "counter = collections.Counter(train_text)\n",
        "\n",
        "# sort according to decreasing frequency\n",
        "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "# words = list of all the words (in decreasing frequency)\n",
        "items, _ = list(zip(*count_pairs))\n",
        "\n",
        "# make a dictionary with a mapping from each word to an id; word with highest frequency gets lowest id etc.\n",
        "item_to_id = dict(zip(items, range(len(items))))\n",
        "id_to_item = dict(zip(range(len(items)), items))\n",
        "vocab_size = len(item_to_id)\n",
        "\n",
        "# convert the words to indices\n",
        "train_ids_large = [item_to_id[item] for item in train_text]\n",
        "valid_ids_large = [item_to_id[item] for item in valid_text]\n",
        "test_ids_large = [item_to_id[item] for item in test_text]\n",
        "\n",
        "# take a smaller subset to speed up training\n",
        "train_ids = train_ids_large[:50000]\n",
        "valid_ids = valid_ids_large[:10000]\n",
        "test_ids = test_ids_large[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6KQShlZFcGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the data is converted to ids, it looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "Vlo5SpAHFhLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e23be379-d350-4dd0-ac93-f11a88a1805d"
      },
      "cell_type": "code",
      "source": [
        "print(valid_ids[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1133, 94, 359, 6, 330, 52, 9837, 7, 327, 2477, 6, 0, 663, 389, 2, 3, 1, 1, 2975, 2159, 10, 382, 1069, 2348, 90, 100, 848, 199, 1, 12, 0, 3384, 1120, 8, 4, 73, 21, 212, 347, 37, 259, 1, 1, 2, 3, 76, 423, 196, 3918, 5, 250, 1796, 1, 581, 3529, 893, 2375, 7, 4, 298, 12, 2710, 17, 1187, 1, 251, 2, 3, 9, 0, 36, 9923, 3748, 465, 711, 2999, 2038, 3918, 135, 6146, 12, 495, 5895, 17, 0, 131, 273, 10, 465, 2, 3, 9959, 733, 504, 31, 642, 7, 36, 6499]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIhqMJ2CKMxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code for building, training and testing neural language models"
      ]
    },
    {
      "metadata": {
        "id": "jBnJVfPuNzmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Class for the language model:"
      ]
    },
    {
      "metadata": {
        "id": "e_IlLSYXN1h7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class rnn_lm(object):\n",
        "  '''\n",
        "  This is a class to build and execute a recurrent neural network language model.\n",
        "  '''\n",
        "  \n",
        "  def __init__(self,\n",
        "              cell='LSTM',\n",
        "              optimizer='SGD',\n",
        "              lr=1,\n",
        "              vocab_size=10000,\n",
        "              embedding_size=64,\n",
        "              hidden_size=128,\n",
        "              dropout_rate=0.5,\n",
        "              is_training=True):\n",
        "    # hyperparameters that can be changed\n",
        "    self.which_cell = cell\n",
        "    self.which_optimizer = optimizer\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.is_training = is_training\n",
        "    self.lr = lr\n",
        "    \n",
        "    # hard-coded hyperparameters\n",
        "    self.batch_size = BATCH_SIZE\n",
        "    self.num_steps = NUM_STEPS\n",
        "    self.max_grad_norm = 5\n",
        "    \n",
        "    \n",
        "    self.init_graph()\n",
        "    \n",
        "    self.output, self.state = self.feed_to_network()\n",
        "    \n",
        "    self.loss = self.calc_loss(self.output)\n",
        "    \n",
        "    if self.is_training:\n",
        "      self.update_params(self.loss)\n",
        "    \n",
        "    \n",
        "  def init_graph(self):\n",
        "    '''\n",
        "    This function initializes all elements of the network.\n",
        "    '''\n",
        "    \n",
        "    self.inputs = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.num_steps])\n",
        "    self.targets = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.num_steps])\n",
        "    \n",
        "    # input embedding weights\n",
        "    self.embedding = tf.get_variable(\"embedding\", \n",
        "                                     [self.vocab_size, self.embedding_size], \n",
        "                                     dtype=tf.float32)\n",
        "    \n",
        "    # hidden layer\n",
        "    if self.which_cell == 'LSTM':\n",
        "      self.basic_cell = tf.contrib.rnn.LSTMCell(self.hidden_size)\n",
        "    elif self.which_cell == 'RNN':\n",
        "      self.basic_cell = tf.contrib.rnn.BasicRNNCell(self.hidden_size)\n",
        "    else:\n",
        "      raise ValueError(\"Specify which type of RNN you want to use: RNN or LSTM.\")\n",
        "      \n",
        "    # apply dropout  \n",
        "    self.cell = tf.contrib.rnn.DropoutWrapper(self.basic_cell, \n",
        "                                              output_keep_prob=self.dropout_rate)\n",
        "    \n",
        "    # initial state contains all zeros\n",
        "    self.initial_state = self.cell.zero_state(self.batch_size, tf.float32)\n",
        "    \n",
        "    # output weight matrix and bias\n",
        "    self.softmax_w = tf.get_variable(\"softmax_w\",\n",
        "                                     [self.hidden_size, self.vocab_size], \n",
        "                                     dtype=tf.float32)\n",
        "    self.softmax_b = tf.get_variable(\"softmax_b\",\n",
        "                                     [self.vocab_size], \n",
        "                                     dtype=tf.float32)\n",
        "    \n",
        "    self.initial_state = self.cell.zero_state(self.batch_size, dtype=tf.float32)\n",
        "    \n",
        "    \n",
        "  def feed_to_network(self):\n",
        "    '''\n",
        "    This function feeds the input to the network and returns the output and the state.\n",
        "   \n",
        "    '''\n",
        "    \n",
        "    # map input indices to continuous input vectors\n",
        "    inputs = tf.nn.embedding_lookup(self.embedding, self.inputs)\n",
        "\n",
        "\t  # use dropout on the input embeddings\n",
        "    inputs = tf.nn.dropout(inputs, self.dropout_rate)\n",
        "    \n",
        "    state = self.initial_state\n",
        "    \n",
        "    # feed inputs to network: outputs = predictions, state = new hidden state\n",
        "    outputs, state = tf.nn.dynamic_rnn(self.cell, inputs, sequence_length=None, initial_state=state)\n",
        "    \n",
        "    output = tf.reshape(tf.concat(outputs, 1), [-1, self.hidden_size])\n",
        "    \n",
        "    return output, state\n",
        "    \n",
        "  \n",
        "  def calc_loss(self, output):\n",
        "    \n",
        "    # calculate logits\n",
        "    # shape of logits = [batch_size*num_steps, vocab_size]\n",
        "    logits = tf.matmul(output, self.softmax_w) + self.softmax_b\n",
        "      \n",
        "    # calculate cross entropy loss\n",
        "    # reshape targets such that it has shape [batch_size*num_steps]\n",
        "    # loss: contains loss for every time step in every batch\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=tf.reshape(self.targets, [-1]), logits=logits)\n",
        "      \n",
        "    # average loss per batch\n",
        "    avg_loss = tf.reduce_sum(loss) / self.batch_size\n",
        "    \n",
        "    return avg_loss\n",
        "  \n",
        "  def update_params(self, loss):\n",
        "    \n",
        "    # calculate gradients for all trainable variables \n",
        "    # + clip them if their global norm > 5 (prevents exploding gradients)\n",
        "    grads, _ = tf.clip_by_global_norm(\n",
        "        tf.gradients(loss, tf.trainable_variables()), self.max_grad_norm)\n",
        "    \n",
        "    if self.which_optimizer == 'SGD':\n",
        "      optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "    elif self.which_optimizer == 'Adam':\n",
        "      optimizer = tf.train.AdamOptimizer(self.lr)\n",
        "    else:\n",
        "      raise ValueError(\"Specify which type of optimizer you want to use: SGD or Adam.\")\n",
        "    \n",
        "    # update the weights\n",
        "    self.train_op = optimizer.apply_gradients(\n",
        "\t\t\t\tzip(grads, tf.trainable_variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tIHeGqTCW8J-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a class that will generate mini-batches from the data."
      ]
    },
    {
      "metadata": {
        "id": "sEvlMOEJa4qU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class batchGenerator(object):\n",
        "  '''\n",
        "  This class generates batches for a dataset.\n",
        "  Input argument:\n",
        "    data: list of indices (word ids)\n",
        "  '''\n",
        "  \n",
        "  def __init__(self, data):\n",
        "    '''\n",
        "    Prepares a dataset.\n",
        "    '''\n",
        "  \n",
        "    data_array = np.array(data)\n",
        "\n",
        "    len_batch_instance = len(data) / BATCH_SIZE\n",
        "\n",
        "    data_array = data_array[:BATCH_SIZE*len_batch_instance]\n",
        "\n",
        "    # divide data in BATCH_SIZE parts\n",
        "    self.data_reshaped = np.reshape(data_array, (BATCH_SIZE, len_batch_instance))\n",
        "\n",
        "    # number of mini-batches that can be generated\n",
        "    self.num_batches_in_data = len_batch_instance / NUM_STEPS - 1\n",
        "    \n",
        "    self.curr_idx = 0\n",
        "  \n",
        "  def generate(self):\n",
        "    '''\n",
        "    Generates\n",
        "      input_batch: numpy array or None, if the end of the dataset is reached\n",
        "      target_batch: numpy array or None, if the end of the dataset is reached\n",
        "      end_reached: boolean, True is end of dataset is reached\n",
        "    '''\n",
        "    \n",
        "    if self.curr_idx >= self.num_batches_in_data:\n",
        "      return None, None, True\n",
        "\n",
        "    # input: take slice of size BATCH_\n",
        "    input_batch = self.data_reshaped[:,self.curr_idx*BATCH_SIZE:self.curr_idx*BATCH_SIZE+BATCH_SIZE]\n",
        "    \n",
        "    # target = input shifted 1 time step\n",
        "    target_batch = self.data_reshaped[:,self.curr_idx*BATCH_SIZE+1:self.curr_idx*BATCH_SIZE+BATCH_SIZE+1]    \n",
        "\n",
        "    self.curr_idx += 1\n",
        "    \n",
        "    return input_batch, target_batch, False\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_SB0UTRdnU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is an example of how batchGenerator can be used. You will notice that the target batch contains the same indices as the input batch, but shifted one (time) step to the right."
      ]
    },
    {
      "metadata": {
        "id": "qHiY_eJ0dpUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1411
        },
        "outputId": "7d6c8deb-5652-4f68-db8b-8b6d07ab3ea6"
      },
      "cell_type": "code",
      "source": [
        "generator = batchGenerator(valid_ids)\n",
        "input_batch, target_batch, end_reached = generator.generate()\n",
        "print('This is what an input batch looks like:\\n{0}'.format(input_batch))\n",
        "print('And this is what a target batch looks like:\\n{0}'.format(target_batch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is what an input batch looks like:\n",
            "[[   2 1133   94  359    6  330   52 9837    7  327 2477    6    0  663\n",
            "   389    2    3    1    1 2975]\n",
            " [  30   15   10 1540    6   26   44    4    4  626 2039    1  173    2\n",
            "     3   65   47  584    6  189]\n",
            " [   1   95 2469   11  390    1   47  214 9936   20    1   80   26 1950\n",
            "    67    0    1    5    1  175]\n",
            " [ 146 2370   16    2    3  640  748 3382 4740 2785   57  336  562    8\n",
            "  1120   23    7   13    4   49]\n",
            " [  47 3280   11   24 2785   37   55    5    0   62 1379 1557  280    1\n",
            "    23    0  948    8 6393   37]\n",
            " [  15  176   33 1056    6  330  122 1296   28    1 1853    8  133 8666\n",
            "  9744    2    3   68   24 2670]\n",
            " [1094  485    1 3218   94   26  659    6  621   11 1030   30 1371   37\n",
            "  6552 8336  128   20   39    1]\n",
            " [   2    3   36  632 4168   83 3089   96 3254 2369    0  695  904   12\n",
            "    28  463    5 1768    1    1]\n",
            " [ 607  363    2    3    1 2848   10    1 3404  193    7 5443    2    3\n",
            "   190    1   35    6   26   75]\n",
            " [   6  300   30    0 2035 1205   17    0  911 7360  160  128 1155   15\n",
            "     2    3  111  379 2932   51]\n",
            " [3059   70   41   68  229    1   47  214 6300    2    3 1090  167   11\n",
            "   846 9920   53 2932   10 7448]\n",
            " [1558 6121   37 5187 7705   35 1899  361    9    0   36  839   27 8690\n",
            "   946   57  160  162    8    0]\n",
            " [ 442   23 9976    2    3   12 1224   29  595    7   13    4   22  502\n",
            "    21    0 3414 3397   19    7]\n",
            " [1390 9896    9  681    0  599 2272  680   19    0 1860    2    3   17\n",
            "  9758   29  490    7  242  116]\n",
            " [ 288  489   31    4   22  247   72  547    2    3    1  720   51   16\n",
            "    15  955    6 3751   72   30]\n",
            " [5319   48    2    3    0   61   48 8597   33  325    6   88  122    5\n",
            "   997   12    7  121   45  401]\n",
            " [ 242  208   37  891  137   80  379   52   72  461  290   46   56    4\n",
            "     4    2    3    0 2763   17]\n",
            " [   8  271   21 2165    1  645    2    3    0  213    8    7    4 1230\n",
            "  5857   85    7 3167  389   18]\n",
            " [   9  289  474    2    3   24    1   25  233   71    9  277  353  572\n",
            "     5    1 3050    1   81    7]\n",
            " [1836 6986    4   73  394   71    9  152  155  291    5 5411  931    5\n",
            "   541   81   25 1235    7  277]]\n",
            "And this is what a target batch looks like:\n",
            "[[1133   94  359    6  330   52 9837    7  327 2477    6    0  663  389\n",
            "     2    3    1    1 2975 2159]\n",
            " [  15   10 1540    6   26   44    4    4  626 2039    1  173    2    3\n",
            "    65   47  584    6  189 1699]\n",
            " [  95 2469   11  390    1   47  214 9936   20    1   80   26 1950   67\n",
            "     0    1    5    1  175    1]\n",
            " [2370   16    2    3  640  748 3382 4740 2785   57  336  562    8 1120\n",
            "    23    7   13    4   49  258]\n",
            " [3280   11   24 2785   37   55    5    0   62 1379 1557  280    1   23\n",
            "     0  948    8 6393   37   11]\n",
            " [ 176   33 1056    6  330  122 1296   28    1 1853    8  133 8666 9744\n",
            "     2    3   68   24 2670 5987]\n",
            " [ 485    1 3218   94   26  659    6  621   11 1030   30 1371   37 6552\n",
            "  8336  128   20   39    1    8]\n",
            " [   3   36  632 4168   83 3089   96 3254 2369    0  695  904   12   28\n",
            "   463    5 1768    1    1    1]\n",
            " [ 363    2    3    1 2848   10    1 3404  193    7 5443    2    3  190\n",
            "     1   35    6   26   75    6]\n",
            " [ 300   30    0 2035 1205   17    0  911 7360  160  128 1155   15    2\n",
            "     3  111  379 2932   51   40]\n",
            " [  70   41   68  229    1   47  214 6300    2    3 1090  167   11  846\n",
            "  9920   53 2932   10 7448    2]\n",
            " [6121   37 5187 7705   35 1899  361    9    0   36  839   27 8690  946\n",
            "    57  160  162    8    0   61]\n",
            " [  23 9976    2    3   12 1224   29  595    7   13    4   22  502   21\n",
            "     0 3414 3397   19    7    1]\n",
            " [9896    9  681    0  599 2272  680   19    0 1860    2    3   17 9758\n",
            "    29  490    7  242  116   24]\n",
            " [ 489   31    4   22  247   72  547    2    3    1  720   51   16   15\n",
            "   955    6 3751   72   30  144]\n",
            " [  48    2    3    0   61   48 8597   33  325    6   88  122    5  997\n",
            "    12    7  121   45  401    1]\n",
            " [ 208   37  891  137   80  379   52   72  461  290   46   56    4    4\n",
            "     2    3    0 2763   17  331]\n",
            " [ 271   21 2165    1  645    2    3    0  213    8    7    4 1230 5857\n",
            "    85    7 3167  389   18    0]\n",
            " [ 289  474    2    3   24    1   25  233   71    9  277  353  572    5\n",
            "     1 3050    1   81    7 2113]\n",
            " [6986    4   73  394   71    9  152  155  291    5 5411  931    5  541\n",
            "    81   25 1235    7  277    5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vBbmu8MyeRjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a function that does one pass over the whole dataset. If we are training the model, it will update the parameters and return the perplexity. Otherwise, it will just return the perplexity."
      ]
    },
    {
      "metadata": {
        "id": "u0QxAecTZIOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(session, rnn, data, is_training=True):\n",
        "    '''\n",
        "    This function runs a single epoch (pass) over the data,\n",
        "    updating the model parameters if we are training,\n",
        "    and returns the perplexity.\n",
        "    Input arguments:\n",
        "      rnn: object of the rnn_lm class\n",
        "      data: list of word indices\n",
        "      is_training: boolean, True is we are training the model\n",
        "    Returns:\n",
        "      ppl: float, perplexity of the dataset\n",
        "    '''\n",
        "  \n",
        "    generator = batchGenerator(data)\n",
        "      \n",
        "    state = session.run(rnn.initial_state)\n",
        "    sum_loss = 0.0\n",
        "    iters = 0\n",
        "      \n",
        "    while True:\n",
        "\n",
        "      input_batch, target_batch, end_reached = generator.generate()\n",
        "        \n",
        "      if end_reached:\n",
        "        break\n",
        "\n",
        "      feed_dict = {rnn.inputs: input_batch,\n",
        "                  rnn.targets: target_batch,\n",
        "                  rnn.initial_state : state}\n",
        "\n",
        "      fetches = {'loss': rnn.loss,\n",
        "                'state': rnn.state}\n",
        "      \n",
        "      if is_training:\n",
        "        fetches['train_op'] = rnn.train_op\n",
        "\n",
        "      result = session.run(fetches, feed_dict)\n",
        "        \n",
        "      state = result['state']\n",
        "      loss = result['loss']\n",
        "\n",
        "      sum_loss += loss\n",
        "      # the loss is an average over num_steps\n",
        "      iters += NUM_STEPS\n",
        "        \n",
        "    # calculate perplexity    \n",
        "    ppl = np.exp(sum_loss / iters)\n",
        "    \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CY4KxCsUenOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This function can be called to build, train and test models with different parameter settings. "
      ]
    },
    {
      "metadata": {
        "id": "5K8dJkswaT7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_lm(cell='LSTM', optimizer='SGD', lr=1, \n",
        "           embedding_size=64, hidden_size=128, \n",
        "           dropout_rate=0.5, inspect_emb=False,\n",
        "           large_data=False):\n",
        "  '''\n",
        "  Creates training, validation and/or test models,\n",
        "  trains, validates and/or tests the model.\n",
        "  '''\n",
        "  \n",
        "  if large_data:\n",
        "    train_ids = train_ids_large\n",
        "    valid_ids = valid_ids_large\n",
        "    test_ids = test_ids_large\n",
        "  \n",
        "  with tf.Graph().as_default():\n",
        "\n",
        "      # create the models\n",
        "      with tf.variable_scope(\"Model\"):\n",
        "        rnn_train = rnn_lm(cell=cell,\n",
        "                           optimizer=optimizer, \n",
        "                           lr=lr,\n",
        "                           vocab_size=vocab_size,\n",
        "                           embedding_size=embedding_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "        \n",
        "        saver = tf.train.Saver()\n",
        "        \n",
        "      with tf.variable_scope(\"Model\", reuse=True):\n",
        "        rnn_valid = rnn_lm(cell=cell, \n",
        "                           optimizer=optimizer,\n",
        "                           lr=lr,\n",
        "                           vocab_size=vocab_size, \n",
        "                           embedding_size=embedding_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           dropout_rate=dropout_rate,\n",
        "                           is_training=False)\n",
        "        \n",
        "      with tf.variable_scope(\"Model\", reuse=True):\n",
        "        rnn_test = rnn_lm(cell=cell, \n",
        "                           optimizer=optimizer, \n",
        "                           lr=lr,\n",
        "                           vocab_size=vocab_size,\n",
        "                           embedding_size=embedding_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           dropout_rate=dropout_rate,\n",
        "                           is_training=False)\n",
        "      \n",
        "\n",
        "      sv = tf.train.Supervisor()\n",
        "\n",
        "      with sv.managed_session(config=tf.ConfigProto()) as session:\n",
        "        \n",
        "        for i in xrange(5):\n",
        "          \n",
        "          print('Epoch {0}'.format(i+1))\n",
        "\n",
        "          train_ppl = run_epoch(session, rnn_train, train_ids)\n",
        "          print('Train perplexity: {0}'.format(train_ppl))\n",
        "\n",
        "          valid_ppl = run_epoch(session, rnn_valid, valid_ids, is_training=False)\n",
        "          print('Validation perplexity: {0}'.format(valid_ppl))\n",
        "          \n",
        "        save_path = saver.save(session, \"models/rnn.ckpt\")\n",
        "        print('Saved the model to ',save_path)\n",
        "\n",
        "        test_ppl = run_epoch(session, rnn_test, test_ids, is_training=False)\n",
        "        print('Test perplexity: {0}'.format(test_ppl))\n",
        "        \n",
        "        if inspect_emb: \n",
        "          emb_matrix = tf.get_default_graph().get_tensor_by_name(\"Model/embedding:0\")\n",
        "          emb_matrix_np = emb_matrix.eval(session=session)\n",
        "\n",
        "          return emb_matrix_np\n",
        "\n",
        "        else:\n",
        "\n",
        "          return None\n",
        "\n",
        "    \n",
        "        \n",
        "        \n",
        "  \n",
        "        \n",
        "      \n",
        "      \n",
        "        \n",
        "     \n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbEp9tdFKXA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training networks"
      ]
    },
    {
      "metadata": {
        "id": "a61RdtDCKhmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training neural networks requires a lot of hyperparameter tuning. The hyperparameters of a neural network are for example the type of cell, its size, the method that is used for updating its parameters (also called 'optimizer' ), the type and strength of regularization, ... . All these hyperparameters have to be chosen before the network can built, trained and tested, and they all have to some extent an influence on the  performance of the model."
      ]
    },
    {
      "metadata": {
        "id": "j3bN2Mvmo_8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks are neural networks that take as input a combination of the standard input and the hidden state of the previous time step. Let's first train a simple recurrent neural network (RNN) as a language model. "
      ]
    },
    {
      "metadata": {
        "id": "zI30kaseLmVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "42u843fb9qq2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now train a simple RNN as language model."
      ]
    },
    {
      "metadata": {
        "id": "KRUa0rDYc7SC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "da37ad5f-526d-434e-b2e2-8f790ae15f76"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 28579.2624974\n",
            "valid_ppl: 7976.3365824\n",
            "Epoch 2\n",
            "train_ppl: 10458.1135736\n",
            "valid_ppl: 1364.88939657\n",
            "Epoch 3\n",
            "train_ppl: 1536.37432389\n",
            "valid_ppl: 2130.04265576\n",
            "Epoch 4\n",
            "train_ppl: 870.760132806\n",
            "valid_ppl: 709.111377606\n",
            "Epoch 5\n",
            "train_ppl: 713.030860672\n",
            "valid_ppl: 724.993998427\n",
            "test_ppl: 689.391726604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "osTYguHZprBE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You see that both the training perplexity and the validation perplexity decreased during training, which is a good sign. However, notice that the validation perplexity of epoch 5 is slightly higher than the validation perplexity of epoch 4. \n"
      ]
    },
    {
      "metadata": {
        "id": "dWILisHi1xHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "22aad054-4419-4398-82b7-218e5dfd58df"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 8.3480932363e+189\n",
            "valid_ppl: 8.94273109753e+240\n",
            "Epoch 2\n",
            "train_ppl: 8.90212615289e+269\n",
            "valid_ppl: 2.70133700595e+271\n",
            "Epoch 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_ppl: inf\n",
            "valid_ppl: inf\n",
            "Epoch 4\n",
            "train_ppl: inf\n",
            "valid_ppl: inf\n",
            "Epoch 5\n",
            "train_ppl: inf\n",
            "valid_ppl: inf\n",
            "test_ppl: inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8wKt7L9aPLfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learning rate"
      ]
    },
    {
      "metadata": {
        "id": "EXDniSW54NfB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Judging from the perplexities above, it seems like the Adam optimizer is a bad choice for training our network! However, the interplay between the different hyperparameters of a neural network is complicated, and it is very well possible that a specific optimizer needs a different learning rate. \n",
        "Let's try a learning rate of 0.01 instead of 1:"
      ]
    },
    {
      "metadata": {
        "id": "pPl3KiSs5kQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6c9b9d68-85c8-4af6-f42a-7658ff7e2b0d"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam', lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 1057.75617242\n",
            "valid_ppl: 958.832506035\n",
            "Epoch 2\n",
            "train_ppl: 564.324279113\n",
            "valid_ppl: 826.433342462\n",
            "Epoch 3\n",
            "train_ppl: 433.956799222\n",
            "valid_ppl: 793.489183177\n",
            "Epoch 4\n",
            "train_ppl: 364.952622399\n",
            "valid_ppl: 723.403317321\n",
            "Epoch 5\n",
            "train_ppl: 324.387741371\n",
            "valid_ppl: 699.624224383\n",
            "test_ppl: 532.177735851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcNjIbzgDUuk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This time, the network is converging nicely. Maybe reducing the learning rate even further helps? Let's try a learning rate of 0.001:"
      ]
    },
    {
      "metadata": {
        "id": "UyUTwGvF7Iyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "bbccda5f-093f-497f-e27a-1db96cec8ba2"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam', lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 647.90650982\n",
            "valid_ppl: 490.249928851\n",
            "Epoch 2\n",
            "train_ppl: 378.543344185\n",
            "valid_ppl: 413.048300797\n",
            "Epoch 3\n",
            "train_ppl: 300.501997416\n",
            "valid_ppl: 375.886351108\n",
            "Epoch 4\n",
            "train_ppl: 258.467386044\n",
            "valid_ppl: 367.867747945\n",
            "Epoch 5\n",
            "train_ppl: 232.486608834\n",
            "valid_ppl: 356.596671902\n",
            "test_ppl: 286.110701589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nNkyWjQNCjja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see an additional improvement. Let's see what reducing the learning rate even further, to 0.0001, gives:"
      ]
    },
    {
      "metadata": {
        "id": "Zn3TWmSz-SyN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5299fffd-6aec-4932-c3f2-22bce284dd0a"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam', lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 1366.65057312\n",
            "valid_ppl: 677.639747264\n",
            "Epoch 2\n",
            "train_ppl: 630.575866312\n",
            "valid_ppl: 690.140848653\n",
            "Epoch 3\n",
            "train_ppl: 625.483174618\n",
            "valid_ppl: 707.694743583\n",
            "Epoch 4\n",
            "train_ppl: 620.683702468\n",
            "valid_ppl: 1629.80400103\n",
            "Epoch 5\n",
            "train_ppl: 668.006540818\n",
            "valid_ppl: 1913.05069198\n",
            "test_ppl: 1789.96267687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6FQejsJzC4TO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we see an interesting result: the training perplexity decreased between epoch 0 and 4, but the validation perplexity is continuously increasing. For epoch 5, even the training perplexity increased again. This is an example of a learning rate that is too small: the steps that the network is making are too small."
      ]
    },
    {
      "metadata": {
        "id": "FdVPv212Lem5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Type of RNN cell"
      ]
    },
    {
      "metadata": {
        "id": "2ZHyiycZCemZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A simple RNN has some disadvantages: it often suffers from the so-called *vanishing and exploding gradients* problem. Neural networks are trained with an algorithm called backpropagation, which computes the gradients of the loss with respect to all parameters in the network. For a language model, the loss of the network is called the *cross entropy*, and it is equal to the average negative log probability for every word in the data. The perplexity of the language model is simply the exponential of the cross entropy. In the case of the simple RNN shown above, the parameters would be the weight matrices $\\mathbf{W}$, $\\mathbf{U}$ and $\\mathbf{V}$ and the bias vectors $\\mathbf{b}$ and $\\mathbf{b_v}$. The gradients of the loss with respect to the parameters $\\mathbf{V}$ and $\\mathbf{b_v}$ can be calculated directly, but the gradients with respect to the other parameters in the network are calculated based on the chain rule, which results in multiplying many terms. Moreover, an RNN is typically *unrolled in time*, which means that you also want to update the weights for the words seen before. If the terms in the multiplication are very small or very  large, they can quickly get even smaller (vanish) or larger (explode). The exploding gradients problem can relatively easily be solved by clipping the (norm of) the gradients if they become too large, the vanishing gradients problem is (at least partially) solved by using another type of RNN cell, such as a long short-term memory (LSTM) cell."
      ]
    },
    {
      "metadata": {
        "id": "L-cN7Ox1_KTT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An LSTM contains two hidden states instead of one, a cell state $\\mathbf{c}_t$ and a hidden state $\\mathbf{h}_t$, and  three gates, the input gate, forget gate and output gate. The gates are shown in the upper part of the figure below: they have a sigmoid activation function, which makes sure that the output values are all between 0 and 1. The forget gate $\\mathbf{f}_t$ is combined with the cell state $\\mathbf{c}_t$: it thus decides which parts of the previous cell state should be forgetten (values close to 0) and which not (values close to 1). A new cell state is then calculated based on a combination of the input gate $\\mathbf{i}_t$, which decides what should be added, and the candidate values $\\mathbf{p}_t$, which are the result of a $tanh$ non-linearity. The new cell state $\\mathbf{c}_t$ is then put through another $tanh$, and combined with the output gate, which decides which part of the input should be let through to the new hidden state $\\mathbf{h}_t$. The last part of the network is the equal to the simple RNN. This [blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) gives a great description of how an LSTM cell works."
      ]
    },
    {
      "metadata": {
        "id": "LA-MvI3y-Yd0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/lverwimp/RNN_language_modeling/blob/master/LSTM.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "oQWpcWb9Hu6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We take the optimal combination of optimizer (Adam) and learning rate (0.001) for an RNN, and use it to train an LSTM:"
      ]
    },
    {
      "metadata": {
        "id": "IoKmC2JwCYhq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0e4e631b-ca0b-420b-dd07-9c7bb54535cb"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 656.82063114\n",
            "valid_ppl: 529.864473663\n",
            "Epoch 2\n",
            "train_ppl: 424.844078727\n",
            "valid_ppl: 464.263171504\n",
            "Epoch 3\n",
            "train_ppl: 356.302922604\n",
            "valid_ppl: 419.632020537\n",
            "Epoch 4\n",
            "train_ppl: 312.305264384\n",
            "valid_ppl: 397.430353409\n",
            "Epoch 5\n",
            "train_ppl: 279.950894164\n",
            "valid_ppl: 376.370915052\n",
            "test_ppl: 308.514639823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AzvS-x9jIKm3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Surprise! The LSTM gives a worse test perplexity, 308.5, than the RNN with the same hyperparameters, 286.1. Looking at the evolution of the perplexities over epochs, we see that they only slowly decrease. Maybe we need a larger learning rate for an LSTM?"
      ]
    },
    {
      "metadata": {
        "id": "lkIJOt8jHoMn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "79f9de2a-f817-44b7-c302-2141a72bb028"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, inspect_emb=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 481.709334113\n",
            "valid_ppl: 396.625967937\n",
            "Epoch 2\n",
            "train_ppl: 286.597902762\n",
            "valid_ppl: 355.204807228\n",
            "Epoch 3\n",
            "train_ppl: 235.102307065\n",
            "valid_ppl: 341.794086337\n",
            "Epoch 4\n",
            "train_ppl: 207.242568642\n",
            "valid_ppl: 341.275447747\n",
            "Epoch 5\n",
            "train_ppl: 188.696659696\n",
            "valid_ppl: 349.23735006\n",
            "test_ppl: 264.915329849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-38546940d49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspect_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-9679138f901c>\u001b[0m in \u001b[0;36mrun_lm\u001b[0;34m(cell, optimizer, lr, embedding_size, hidden_size, dropout_rate, inspect_emb)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minspect_emb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0minspect_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HSv59sc7JW6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This perplexity is already much better. By further optimizing of the learning rate and/or optimizer, we could probably get even lower perplexities."
      ]
    },
    {
      "metadata": {
        "id": "MGAY_hhfNLj5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Size of the embedding"
      ]
    },
    {
      "metadata": {
        "id": "jZ87PmaPJlib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at the influence of the size of the LSTM on its performance. By default, we train a model with embeddings of size 64 and a hidden layer of size 128. Let's see what happens if we reduce the size of the embedding:"
      ]
    },
    {
      "metadata": {
        "id": "q2d_Kx8zJGk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9ad75cc0-6d9a-4337-d957-fbc55eed3f4c"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, embedding_size=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 519.359682418\n",
            "valid_ppl: 5124521.52183\n",
            "Epoch 2\n",
            "train_ppl: 325.127520433\n",
            "valid_ppl: 384.575596662\n",
            "Epoch 3\n",
            "train_ppl: 273.853564489\n",
            "valid_ppl: 366.642785843\n",
            "Epoch 4\n",
            "train_ppl: 245.163913857\n",
            "valid_ppl: 358.26870915\n",
            "Epoch 5\n",
            "train_ppl: 225.340377224\n",
            "valid_ppl: 356.266744414\n",
            "test_ppl: 288.184551098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ranM0ERNIta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Size of the hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "vbkbvBaWLVaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "84dc1cfe-578c-4200-82f6-e8d81afed187"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, hidden_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 522.602473469\n",
            "valid_ppl: 433.018669464\n",
            "Epoch 2\n",
            "train_ppl: 325.534562413\n",
            "valid_ppl: 382.469223656\n",
            "Epoch 3\n",
            "train_ppl: 271.586685795\n",
            "valid_ppl: 365.897337815\n",
            "Epoch 4\n",
            "train_ppl: 241.188401725\n",
            "valid_ppl: 356.627466171\n",
            "Epoch 5\n",
            "train_ppl: 221.587265914\n",
            "valid_ppl: 352.766427407\n",
            "test_ppl: 279.318501236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F8O4aKe2NDBB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Regularization"
      ]
    },
    {
      "metadata": {
        "id": "YFKOAC30M1AA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1fdacc68-ef0f-4818-af8b-50ee50046552"
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, dropout_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 809.953863498\n",
            "valid_ppl: 658.909363669\n",
            "Epoch 2\n",
            "train_ppl: 584.828653476\n",
            "valid_ppl: 606.689601129\n",
            "Epoch 3\n",
            "train_ppl: 539.409788271\n",
            "valid_ppl: 585.649933925\n",
            "Epoch 4\n",
            "train_ppl: 519.498692319\n",
            "valid_ppl: 574.898769121\n",
            "Epoch 5\n",
            "train_ppl: 508.607510944\n",
            "valid_ppl: 574.182146616\n",
            "test_ppl: 517.1558548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UozYDTgTPYBN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "yUR-tGjo7x-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Often the input words for a language model are represented as indices in a vocabulary, or one-hot vectors (where all values are 0 except the index of the word, which has value 1). This representation is a discrete representation, just like in n-gram language models. It has the disadvantage that relationships between words (e.g. the syntactic relationship between 'eat' and 'eating', or the semantic relationship between 'eat' and 'drink') can not be inferred from the word representations. "
      ]
    },
    {
      "metadata": {
        "id": "QNYBzUniRG1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural language models however, do not use this representation as is but first map it to a continuous, lower-dimensional vector, also called *word embedding*. They do this by looking up the index of the word in a weight matrix $\\mathbf{W}$, which is often called the embedding matrix. By training the embedding matrix jointly with the rest of the language model, the resulting word embeddings will have some interesting properties: several syntactic and semantic relationships are encoded as vector offsets in the embedding space. A famous example is the vector offset for male - female, which is shown in the example below:"
      ]
    },
    {
      "metadata": {
        "id": "M3DDEbxR7aka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/lverwimp/RNN_language_modeling/blob/master/kingqueen.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "N2WZsA5RRRYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now train a language model and return the embedding matrix of the trained model:"
      ]
    },
    {
      "metadata": {
        "id": "bQ1ZQsmOBoFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "b8e81bc1-0435-4367-badf-868609b402fa"
      },
      "cell_type": "code",
      "source": [
        "emb_matrix = run_lm(cell='LSTM', optimizer='Adam', lr=0.01, inspect_emb=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-d37e781f884a>:44: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 753.721438166\n",
            "Validation perplexity: 1630.02584043\n",
            "Epoch 2\n",
            "Train perplexity: 393.273645101\n",
            "Validation perplexity: 3314.25356314\n",
            "Epoch 3\n",
            "Train perplexity: 316.631636136\n",
            "Validation perplexity: 612.574324836\n",
            "Epoch 4\n",
            "Train perplexity: 270.262470988\n",
            "Validation perplexity: 644.050838949\n",
            "Epoch 5\n",
            "Train perplexity: 233.543918355\n",
            "Validation perplexity: 642.119771996\n",
            "('Saved the model to ', 'models/rnn.ckpt')\n",
            "Test perplexity: 503.763126583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S3IgXe02RZXC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "tKepamr6R-vX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_closest_words(emb_matrix, word):\n",
        "  if word not in item_to_id:\n",
        "    raise IOError('This item is not in the vocabulary')\n",
        "    \n",
        "  else:\n",
        "    id_w = item_to_id[word]\n",
        "    emb_w = emb_matrix[id_w]\n",
        "    norm_emb_w = emb_w / np.linalg.norm(emb_w)\n",
        "    \n",
        "    top_10 = {}\n",
        "    \n",
        "    # iterate over all words\n",
        "    for idx in range(emb_matrix.shape[0]):\n",
        "      # ignore the word itself\n",
        "      if idx != id_w:\n",
        "        \n",
        "        norm_curr_w = emb_matrix[idx] / np.linalg.norm(emb_matrix[idx])\n",
        "        \n",
        "        cos_sim = np.dot(norm_emb_w, norm_curr_w)\n",
        "        \n",
        "        #cos_sim = np.dot(emb_w, emb_matrix[idx]) / \\\n",
        "        #   norm_emb_w * np.linalg.norm(emb_matrix[idx])\n",
        "        \n",
        "        print('{0}\\t{1}'.format(id_to_item[idx], cos_sim))\n",
        "        \n",
        "        # keep list of top 10 largest cos similarities\n",
        "        if len(top_10) >= 10:\n",
        "          for sim in top_10.iterkeys():\n",
        "            if cos_sim > sim:\n",
        "              \n",
        "              print(cos_sim)\n",
        "              print('add new')\n",
        "              print(top_10)\n",
        "              \n",
        "              del top_10[sim]\n",
        "              top_10[cos_sim] = id_to_item[idx]\n",
        "              break\n",
        "        \n",
        "        else:\n",
        "          top_10[cos_sim] = id_to_item[idx]\n",
        "          \n",
        "        \n",
        "    print('Words with largest cosine similarity w.r.t. {0}'.format(word))\n",
        "    print('Word\\t\\tCosine similarity')\n",
        "    # sort the top 10 \n",
        "    for sim in sorted(top_10, key=float):\n",
        "      print('{0}\\t\\t{1}'.format(top_10[sim], sim))\n",
        "      \n",
        "     \n",
        "    \n",
        " \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4uiZ1zheg2FJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b3f5b280-081e-4556-b7d1-476fc03d1a01"
      },
      "cell_type": "code",
      "source": [
        "#find_closest_words(emb_matrix, 'test')\n",
        "#find_closest_words(emb_matrix, 'cat')\n",
        "\n",
        "np.save('emb_matrix_large.npy', emb_matrix_large)\n",
        "\n",
        "id_cat = item_to_id['cat']\n",
        "emb_cat = emb_matrix[id_cat]\n",
        "norm_emb_cat = emb_cat / np.linalg.norm(emb_cat)\n",
        "\n",
        "id_dog = item_to_id['dog']\n",
        "emb_dog = emb_matrix[id_dog]\n",
        "norm_emb_dog = emb_dog / np.linalg.norm(emb_dog)\n",
        "\n",
        "print(np.dot(norm_emb_cat, norm_emb_dog))\n",
        "\n",
        "id_run = item_to_id['run']\n",
        "emb_run = emb_matrix[id_run]\n",
        "norm_emb_run = emb_run / np.linalg.norm(emb_run)\n",
        "\n",
        "print(np.dot(norm_emb_cat, norm_emb_run))\n",
        "\n",
        "print(emb_matrix)\n",
        "\n",
        "#find_closest_words(emb_matrix, 'running')\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.06565641\n",
            "0.030808263\n",
            "[[ 0.21855688 -0.3873319  -0.00287872 ... -0.324526    0.02802591\n",
            "   0.00973321]\n",
            " [-0.02929914 -0.04990479 -0.08945605 ... -0.03788972  0.18728718\n",
            "   0.0034705 ]\n",
            " [-1.0977641   1.3059888   0.55077195 ...  0.8143192  -0.4408592\n",
            "   0.66048986]\n",
            " ...\n",
            " [ 0.22299272  0.00958521 -0.20285715 ... -0.06741241  0.06712095\n",
            "   0.19544417]\n",
            " [ 0.20967102  0.22128637  0.33368605 ... -0.0949669  -0.38403493\n",
            "  -0.10474928]\n",
            " [-0.2630774   0.04419379  0.02579399 ...  0.10079039 -0.06945877\n",
            "   0.05927106]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AvLOjQy8l_D1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "9f8b12c5-195a-446c-aa6c-34bdff9d450b"
      },
      "cell_type": "code",
      "source": [
        "emb_matrix_large = run_lm(cell='LSTM', optimizer='Adam', lr=0.01, inspect_emb=True, large_data=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 297.18710552\n",
            "Validation perplexity: 234.21438526\n",
            "Epoch 2\n",
            "Train perplexity: 204.754365857\n",
            "Validation perplexity: 213.141694848\n",
            "Epoch 3\n",
            "Train perplexity: 184.758054706\n",
            "Validation perplexity: 204.33167339\n",
            "Epoch 4\n",
            "Train perplexity: 174.919933556\n",
            "Validation perplexity: 200.768920798\n",
            "Epoch 5\n",
            "Train perplexity: 168.839457556\n",
            "Validation perplexity: 199.088644593\n",
            "('Saved the model to ', 'models/rnn.ckpt')\n",
            "Test perplexity: 186.386608675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SvdmjmEJvmTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('emb_matrix_large.npy', emb_matrix_large)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51MqOT86v9nF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "033ed29f-ece1-4eef-f1ff-cd22ba671f8c"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(emb_matrix_large)\n",
        "print(principalComponents)\n",
        "print(principalComponents[:,0].shape)\n",
        "\n",
        "colors = ['navy', 'turquoise', 'darkorange', 'red', 'black', 'blue','yellow','green']\n",
        "target_names = ['cat', 'dog', 'elephant', 'tiger', 'mouse', 'driving','walking','flying']\n",
        "\n",
        "for color, target_name in zip(colors, target_names):\n",
        "    plt.scatter(principalComponents[item_to_id[target_name], 0], \n",
        "                principalComponents[item_to_id[target_name], 1], \n",
        "                color=color, \n",
        "                label=target_name)\n",
        "plt.legend()\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.2317762  -0.28708664]\n",
            " [ 0.44513637  0.10185511]\n",
            " [-4.613992    6.734643  ]\n",
            " ...\n",
            " [ 0.8617239   0.5311671 ]\n",
            " [ 2.5116565  -0.19842397]\n",
            " [-0.7011888  -1.1982691 ]]\n",
            "(10001,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe495f90b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAIIZAAALOD/A/i0A8vLmZ23AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe4962cfe50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}