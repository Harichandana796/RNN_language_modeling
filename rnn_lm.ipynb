{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_lms.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lverwimp/RNN_language_modeling/blob/master/rnn_lm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FyAmshu3bFz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Language Modeling with Recurrent Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "0fc7h-VVbPBV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will see how you can train a recurrent neural network language model.\n",
        "\n",
        "We will start by importing TensorFlow, which is Google's open-source library for machine learning. Next, we will explain how to do data processing for language modeling and show you how we can train and test models."
      ]
    },
    {
      "metadata": {
        "id": "vzV_zLwTbr94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing TensorFlow and other requirements"
      ]
    },
    {
      "metadata": {
        "id": "esuUK2ev_Nyt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We start by importing TensorFlow and checking if we are running on GPU:"
      ]
    },
    {
      "metadata": {
        "id": "_yKJ-Plo_HTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FKKFmkwat9W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the code above raised an error, you should make sure that you are using a GPU in the following way: select 'Runtime' in the top bar, then 'Change runtime type' and choose 'GPU' as hardware accelerator. Training neural networks is much faster on a GPU (graphics processing unit) than on a CPU."
      ]
    },
    {
      "metadata": {
        "id": "OZ6KKnfoahTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we do the other imports we need. The following code will allow you to upload files: you have to upload batchGenerator.py, rnn_lm.py and run_lm.py."
      ]
    },
    {
      "metadata": {
        "id": "BnEd3YIyDsDd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib, collections, os\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILImWZp-_JCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next cells contains some classes and functions that we will use throughout the notebook. You have to run the cells to make sure you can use them, but you do not have to look at the details in the code."
      ]
    },
    {
      "metadata": {
        "id": "bAsnD-TM6qVZ",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class batchGenerator(object):\n",
        "  '''\n",
        "  This class generates batches for a dataset.\n",
        "  Input arguments:\n",
        "    data: list of indices (word ids)\n",
        "    batch_size: number of sequences in a mini-batch\n",
        "    num_steps: length of each sequence in the mini-batch\n",
        "    test: boolean, is True if we are testing; in that case batch_size and num_steps are 1\n",
        "  '''\n",
        "  \n",
        "  def __init__(self, data, batch_size=32, num_steps=50, test=False):\n",
        "    '''\n",
        "    Prepares a dataset.\n",
        "    '''\n",
        "    self.batch_size = batch_size\n",
        "    self.num_steps = num_steps\n",
        "    self.test = test \n",
        "\n",
        "    self.data_array = np.array(data)\n",
        "  \n",
        "    if not self.test:\n",
        "      len_batch_instance = len(data) / batch_size\n",
        "\n",
        "      data_array = self.data_array[:batch_size*len_batch_instance]\n",
        "\n",
        "      # divide data in batch_size parts\n",
        "      self.data_reshaped = np.reshape(data_array, (batch_size, len_batch_instance))\n",
        "\n",
        "      # number of mini-batches that can be generated\n",
        "      self.num_batches_in_data = len_batch_instance / num_steps - 1\n",
        "    \n",
        "    self.curr_idx = 0\n",
        "  \n",
        "  def generate(self):\n",
        "    '''\n",
        "    Generates\n",
        "      input_batch: numpy array (batch_size x num_steps) or None, if the end of the dataset is reached\n",
        "      target_batch: numpy array (batch_size x num_steps) or None, if the end of the dataset is reached\n",
        "      end_reached: boolean, True is end of dataset is reached\n",
        "    '''\n",
        "    \n",
        "    if self.test:\n",
        "      if self.curr_idx+1 >= len(self.data_array):\n",
        "        return None, None, True\n",
        "      \n",
        "      input_batch = [[self.data_array[self.curr_idx]]]\n",
        "      target_batch = [[self.data_array[self.curr_idx+1]]]\n",
        "      \n",
        "    else:\n",
        "      if self.curr_idx >= self.num_batches_in_data:\n",
        "        return None, None, True\n",
        "\n",
        "      # input: take slice of size \n",
        "      input_batch = self.data_reshaped[:,self.curr_idx*self.num_steps:self.curr_idx*self.num_steps+self.num_steps]\n",
        "\n",
        "      # target = input shifted 1 time step\n",
        "      target_batch = self.data_reshaped[:,self.curr_idx*self.num_steps+1:self.curr_idx*self.num_steps+self.num_steps+1]\n",
        "\n",
        "    self.curr_idx += 1\n",
        "    \n",
        "    return input_batch, target_batch, False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aHDgLlvV6sfc",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class rnn_lm(object):\n",
        "  '''\n",
        "  This is a class to build and execute a recurrent neural network language model.\n",
        "  Arguments:\n",
        "    cell: type of RNN cell (only LSTM is currently implemented)\n",
        "    optimizer: 'SGD' or 'Adam'\n",
        "    lr: learning rate\n",
        "    vocab_size: size of the vocabulary\n",
        "    embedding_size: size of continuous embedding that will be input to the RNN\n",
        "    hidden_size: size of the hidden layer\n",
        "    dropout rate: value between 0 and 1, number of neurons that will be \n",
        "        kept (not dropped) during training, prevents overfitting\n",
        "    batch_size: number of sequences that will be input simultaneously\n",
        "    num_steps: length of each input sequence = number of steps in backpropagation through time\n",
        "    is_training: boolean, True is we want to update the parameters of the model\n",
        "  '''\n",
        "  \n",
        "  def __init__(self,\n",
        "              cell='LSTM',\n",
        "              optimizer='Adam',\n",
        "              lr=0.01,\n",
        "              vocab_size=10000,\n",
        "              embedding_size=64,\n",
        "              hidden_size=128,\n",
        "              dropout_rate=0.5,\n",
        "              batch_size=32,\n",
        "              num_steps = 50,\n",
        "              is_training=True):\n",
        "    # hyperparameters that can be changed\n",
        "    self.which_cell = cell\n",
        "    self.which_optimizer = optimizer\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.is_training = is_training\n",
        "    self.lr = lr\n",
        "    self.batch_size = batch_size\n",
        "    self.num_steps = num_steps\n",
        "    \n",
        "    # hard-coded hyperparameters\n",
        "    self.max_grad_norm = 5\n",
        "    \n",
        "    self.init_graph()\n",
        "    \n",
        "    self.output, self.state = self.feed_to_network()\n",
        "    \n",
        "    self.loss = self.calc_loss(self.output)\n",
        "    \n",
        "    if self.is_training:\n",
        "      self.update_params(self.loss)\n",
        "    \n",
        "    \n",
        "  def init_graph(self):\n",
        "    '''\n",
        "    This function initializes all elements of the network.\n",
        "    '''\n",
        "    \n",
        "    self.inputs = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.num_steps])\n",
        "    self.targets = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.num_steps])\n",
        "    \n",
        "    # input embedding weights\n",
        "    self.embedding = tf.get_variable(\"embedding\", \n",
        "                                     [self.vocab_size, self.embedding_size], \n",
        "                                     dtype=tf.float32)\n",
        "    \n",
        "    # hidden layer\n",
        "    if self.which_cell == 'LSTM':\n",
        "      self.basic_cell = tf.contrib.rnn.LSTMCell(self.hidden_size)\n",
        "    elif self.which_cell == 'RNN':\n",
        "      self.basic_cell = tf.contrib.rnn.BasicRNNCell(self.hidden_size)\n",
        "    else:\n",
        "      raise ValueError(\"Specify which type of RNN you want to use: RNN or LSTM.\")\n",
        "      \n",
        "    # apply dropout  \n",
        "    self.cell = tf.contrib.rnn.DropoutWrapper(self.basic_cell, \n",
        "                                              output_keep_prob=self.dropout_rate)\n",
        "    \n",
        "    # initial state contains all zeros\n",
        "    self.initial_state = self.cell.zero_state(self.batch_size, tf.float32)\n",
        "    \n",
        "    # output weight matrix and bias\n",
        "    self.softmax_w = tf.get_variable(\"softmax_w\",\n",
        "                                     [self.hidden_size, self.vocab_size], \n",
        "                                     dtype=tf.float32)\n",
        "    self.softmax_b = tf.get_variable(\"softmax_b\",\n",
        "                                     [self.vocab_size], \n",
        "                                     dtype=tf.float32)\n",
        "    \n",
        "    self.initial_state = self.cell.zero_state(self.batch_size, dtype=tf.float32)\n",
        "    \n",
        "    \n",
        "  def feed_to_network(self):\n",
        "    '''\n",
        "    This function feeds the input to the network and returns the output and the state.\n",
        "   \n",
        "    '''\n",
        "    \n",
        "    # map input indices to continuous input vectors\n",
        "    inputs = tf.nn.embedding_lookup(self.embedding, self.inputs)\n",
        "\n",
        "\t  # use dropout on the input embeddings\n",
        "    inputs = tf.nn.dropout(inputs, self.dropout_rate)\n",
        "    \n",
        "    state = self.initial_state\n",
        "    \n",
        "    # feed inputs to network: outputs = predictions, state = new hidden state\n",
        "    outputs, state = tf.nn.dynamic_rnn(self.cell, inputs, sequence_length=None, initial_state=state)\n",
        "    \n",
        "    output = tf.reshape(tf.concat(outputs, 1), [-1, self.hidden_size])\n",
        "    \n",
        "    return output, state\n",
        "    \n",
        "  \n",
        "  def calc_loss(self, output):\n",
        "    \n",
        "    # calculate logits\n",
        "    # shape of logits = [batch_size*num_steps, vocab_size]\n",
        "    logits = tf.matmul(output, self.softmax_w) + self.softmax_b\n",
        "    \n",
        "    self.softmax = tf.nn.softmax(logits)\n",
        "      \n",
        "    # calculate cross entropy loss\n",
        "    # reshape targets such that it has shape [batch_size*num_steps]\n",
        "    # loss: contains loss for every time step in every batch\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=tf.reshape(self.targets, [-1]), logits=logits)\n",
        "      \n",
        "    # average loss per batch\n",
        "    avg_loss = tf.reduce_sum(loss) / self.batch_size\n",
        "    \n",
        "    return avg_loss\n",
        "  \n",
        "  def update_params(self, loss):\n",
        "    \n",
        "    # calculate gradients for all trainable variables \n",
        "    # + clip them if their global norm > 5 (prevents exploding gradients)\n",
        "    grads, _ = tf.clip_by_global_norm(\n",
        "        tf.gradients(loss, tf.trainable_variables()), self.max_grad_norm)\n",
        "    \n",
        "    if self.which_optimizer == 'SGD':\n",
        "      optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "    elif self.which_optimizer == 'Adam':\n",
        "      optimizer = tf.train.AdamOptimizer(self.lr)\n",
        "    else:\n",
        "      raise ValueError(\"Specify which type of optimizer you want to use: SGD or Adam.\")\n",
        "    \n",
        "    # update the weights\n",
        "    self.train_op = optimizer.apply_gradients(\n",
        "\t\t\t\tzip(grads, tf.trainable_variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ABVaoeuv6xdY",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def run_lm(name='LSTM', cell='LSTM', \n",
        "           optimizer='Adam', lr=0.01, \n",
        "           vocab_size = 10000, embedding_size=64, \n",
        "           hidden_size=128, dropout_rate=0.5, \n",
        "           num_steps=50, inspect_emb=False, \n",
        "           train_ids=None, valid_ids=None, \n",
        "           test_ids=None, test_log_prob=False):\n",
        "  '''\n",
        "  Creates training, validation and/or test models,\n",
        "  trains, validates and/or tests the model.\n",
        "  Arguments:\n",
        "    name: name that will be used to save the model\n",
        "    cell: type of RNN cell (only LSTM is currently implemented)\n",
        "    optimizer: 'SGD' or 'Adam'\n",
        "    lr: learning rate\n",
        "    vocab_size: size of the vocabulary\n",
        "    embedding_size: size of continuous embedding that will be input to the RNN\n",
        "    hidden_size: size of the hidden layer\n",
        "    dropout rate: value between 0 and 1, number of neurons that will be \n",
        "        kept (not dropped) during training, prevents overfitting\n",
        "    num_steps\n",
        "    inspect_emb: boolean, if True we want to return the embedding_matrix\n",
        "    train_ids: training data\n",
        "    valid_ids: validation data\n",
        "    test_ids: test data\n",
        "    test_log_prob: boolean, if True we only want to test the log probability for a test sentence\n",
        "  '''\n",
        "    \n",
        "  with tf.Graph().as_default() as graph:\n",
        "\n",
        "      # create the models\n",
        "      if not test_log_prob:\n",
        "      \n",
        "        with tf.variable_scope(\"Model\"):\n",
        "          rnn_train = rnn_lm(cell=cell,\n",
        "                             optimizer=optimizer, \n",
        "                             lr=lr,\n",
        "                             vocab_size=vocab_size,\n",
        "                             embedding_size=embedding_size,\n",
        "                             hidden_size=hidden_size,\n",
        "                             dropout_rate=dropout_rate)\n",
        "\n",
        "          saver = tf.train.Saver()\n",
        "\n",
        "        with tf.variable_scope(\"Model\", reuse=True):\n",
        "          rnn_valid = rnn_lm(cell=cell, \n",
        "                             optimizer=optimizer,\n",
        "                             lr=lr,\n",
        "                             vocab_size=vocab_size, \n",
        "                             embedding_size=embedding_size,\n",
        "                             hidden_size=hidden_size,\n",
        "                             dropout_rate=dropout_rate,\n",
        "                             is_training=False)\n",
        "          \n",
        "        reuse = True\n",
        "        \n",
        "      else:\n",
        "        reuse = False\n",
        "               \n",
        "      with tf.variable_scope(\"Model\", reuse=reuse):\n",
        "        rnn_test = rnn_lm(cell=cell, \n",
        "                           optimizer=optimizer, \n",
        "                           lr=lr,\n",
        "                           vocab_size=vocab_size,\n",
        "                           embedding_size=embedding_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           dropout_rate=dropout_rate,\n",
        "                           batch_size=1,\n",
        "                           num_steps=1,\n",
        "                           is_training=False)\n",
        "      \n",
        "\n",
        "      sv = tf.train.Supervisor(logdir=name)\n",
        "\n",
        "      with sv.managed_session(config=tf.ConfigProto()) as session:\n",
        "        \n",
        "        if not test_log_prob:\n",
        "        \n",
        "          for i in xrange(5):\n",
        "\n",
        "            print('Epoch {0}'.format(i+1))\n",
        "\n",
        "            train_ppl = run_epoch(session, rnn_train, train_ids, num_steps=num_steps)\n",
        "            print('Train perplexity: {0}'.format(train_ppl))\n",
        "\n",
        "            valid_ppl = run_epoch(session, rnn_valid, valid_ids, num_steps=num_steps, is_training=False)\n",
        "            print('Validation perplexity: {0}'.format(valid_ppl))\n",
        "\n",
        "          save_path = saver.save(session, \"{0}/rnn.ckpt\".format(name))\n",
        "          print('Saved the model to ',save_path)\n",
        "\n",
        "        test_ppl = run_epoch(session, rnn_test, test_ids, num_steps=num_steps,\n",
        "                             is_training=False, is_test=True, \n",
        "                             test_log_prob=test_log_prob)\n",
        "        if not test_log_prob:\n",
        "          print('Test perplexity: {0}'.format(test_ppl))\n",
        "        \n",
        "        if inspect_emb: \n",
        "          emb_matrix = tf.get_default_graph().get_tensor_by_name(\"Model/embedding:0\")\n",
        "          emb_matrix_np = emb_matrix.eval(session=session)\n",
        "\n",
        "          return emb_matrix_np\n",
        "\n",
        "        else:\n",
        "\n",
        "          return None\n",
        "\n",
        "def run_epoch(session, rnn, data, num_steps=50, is_training=True, is_test=False, test_log_prob=False):\n",
        "    '''\n",
        "    This function runs a single epoch (pass) over the data,\n",
        "    updating the model parameters if we are training,\n",
        "    and returns the perplexity.\n",
        "    Input arguments:\n",
        "      rnn: object of the rnn_lm class\n",
        "      data: list of word indices\n",
        "      num_steps\n",
        "      is_training: boolean, True is we are training the model\n",
        "      is_test: boolean, True is we are testing a trained model\n",
        "      test_log_prob: boolean, True if we want the log probability\n",
        "    Returns:\n",
        "      ppl: float, perplexity of the dataset\n",
        "    '''\n",
        "  \n",
        "    generator = batchGenerator(data, test=is_test)\n",
        "      \n",
        "    state = session.run(rnn.initial_state)\n",
        "    sum_loss = 0.0\n",
        "    iters = 0\n",
        "    \n",
        "    if test_log_prob: \n",
        "      sum_log_prob = 0.0\n",
        "      \n",
        "    while True:\n",
        "\n",
        "      input_batch, target_batch, end_reached = generator.generate()\n",
        "        \n",
        "      if end_reached:\n",
        "        break\n",
        "\n",
        "      feed_dict = {rnn.inputs: input_batch,\n",
        "                  rnn.targets: target_batch,\n",
        "                  rnn.initial_state : state}\n",
        "\n",
        "      fetches = {'loss': rnn.loss,\n",
        "                'state': rnn.state}\n",
        "      \n",
        "      if is_training:\n",
        "        fetches['train_op'] = rnn.train_op\n",
        "        \n",
        "      if test_log_prob:\n",
        "        fetches['softmax'] = rnn.softmax\n",
        "        \n",
        "      result = session.run(fetches, feed_dict)\n",
        "        \n",
        "      state = result['state']\n",
        "      loss = result['loss']\n",
        "      \n",
        "      if test_log_prob:\n",
        "        softmax = result['softmax']\n",
        "        prob_target = softmax[0][target_batch[0][0]]\n",
        "        sum_log_prob += np.log(prob_target)\n",
        "\n",
        "      sum_loss += loss\n",
        "      # the loss is an average over num_steps\n",
        "      if is_test:\n",
        "        iters += 1\n",
        "      else:\n",
        "        iters += num_steps\n",
        "        \n",
        "    # calculate perplexity    \n",
        "    ppl = np.exp(sum_loss / iters)\n",
        "    \n",
        "    if test_log_prob:\n",
        "      print('Log probability: {0}'.format(sum_log_prob))\n",
        "    \n",
        "    return ppl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o3-bMavvngQx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you have run all cells in this section, you can now start the following section on data processing."
      ]
    },
    {
      "metadata": {
        "id": "7hmCKPx4KEcG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ]
    },
    {
      "metadata": {
        "id": "-oKtBwJQAJED",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will train our language models on **Penn TreeBank**, which is a publicly available benchmark dataset. A benchmark dataset can be used to easily compare models, since everyone has access to the same data. Many published papers use Penn TreeBank as dataset.\n",
        "\n",
        "It consists of among others newspaper articles, transcribed telephone conversations and manuals. The training set contains ca. 900.000 words, the validation set ca. 70.000 words and the test set ca. 80k words. This is a very small dataset (nowadays language models can be trained on billions of words), but it is large enough for our purposes.\n",
        "\n",
        "We now download the training, validation and test data:"
      ]
    },
    {
      "metadata": {
        "id": "t6AIeiR2BaMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_url = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/data/train.txt'\n",
        "valid_url = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/data/valid.txt'\n",
        "test_url = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/data/test.txt'\n",
        "train_file = urllib.urlopen(train_url).read()\n",
        "valid_file = urllib.urlopen(valid_url).read()\n",
        "test_file = urllib.urlopen(test_url).read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Ye3oJavH-8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "GXEcoaWKIAtE",
        "colab_type": "code",
        "outputId": "937a832a-54c6-48aa-e2f6-b75fb21a37d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "print('{0}...'.format(valid_file[:500]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " consumers may want to move their telephones a little closer to the tv set \n",
            " <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            " two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues \n",
            " and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day 's show \n",
            " interactive telephone technology...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ZF1PoUJDRkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data has been **normalized**: all words not in the vocabulary are mapped to an unknown words class (<unk\\>), all numbers are mapped to the 'N' class, each line contains a single sentence, punctuation has been removed, and so on. \n",
        "\n",
        "The purpose of normalization is among others to get rid of all information that is not necessary (such as punctuation), to solve redundancies (for example the same word can occur with different spellings, e.g. 'normalisation' or 'normalization', and we want to get rid of such variants) and to make sure the language model will be able to generalize better. An example of the latter case is the mapping of all numbers to 'N':  in the example above, 'in N years', 'N' can correspond to any number. Assume that in our training data, we see 'in 20 years' and 'in 11 years', and in our test data, we see 'in 5 years'. If '20', '11' and '5' are not mapped to 'N', we have never seen 'in 5 years' before, and the probability estimate will be worse.\n",
        "  \n",
        "We will now read the data, add end-of-sentence symbols (since we want to be able to predict the end of a sentence too), and count the frequency of every word in the training data:"
      ]
    },
    {
      "metadata": {
        "id": "Y531EV5aDVd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert the string to a list and replace newlines with the end-of-sentence symbol <eos>\n",
        "# ignore empty elements ''\n",
        "train_text = [w for w in train_file.replace('\\n',' <eos>').split(' ') if w != '']\n",
        "valid_text = [w for w in valid_file.replace('\\n',' <eos>').split(' ') if w != '']\n",
        "test_text = [w for w in test_file.replace('\\n',' <eos>').split(' ') if w != '']\n",
        "\n",
        "# count the frequencies of the words in the training data\n",
        "counter = collections.Counter(train_text)\n",
        "\n",
        "# sort according to decreasing frequency\n",
        "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ci3iri5qAu2n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can take a look at the frequencies of the words in the training set, and compare them with the frequencies of the words in the validation set. The top 20 words is quite similar:"
      ]
    },
    {
      "metadata": {
        "id": "8AC1Y_TxAHYK",
        "colab_type": "code",
        "outputId": "dc79f744-2653-4198-a99f-e7592dc89684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "# count the frequencies of the words in the validation data\n",
        "counter_valid = collections.Counter(valid_text)\n",
        "\n",
        "# sort according to decreasing frequency\n",
        "count_pairs_valid = sorted(counter_valid.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "print('Top 20 most frequent words:')\n",
        "print('Train (freq.)\\t\\tValid (freq.)')\n",
        "# we can take a look a the 20 most frequent words + their frequencies:\n",
        "for i in range(20):\n",
        "  print('{0} ({1})\\t\\t{2} ({3})'.format(count_pairs[i][0],count_pairs[i][1],count_pairs_valid[i][0],count_pairs_valid[i][1]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 20 most frequent words:\n",
            "Train (freq.)\t\tValid (freq.)\n",
            "the (50770)\t\tthe (4122)\n",
            "<unk> (45020)\t\t<unk> (3485)\n",
            "<eos> (42068)\t\t<eos> (3370)\n",
            "N (32481)\t\tN (2603)\n",
            "of (24400)\t\tof (1832)\n",
            "to (23638)\t\tto (1750)\n",
            "a (21196)\t\ta (1738)\n",
            "in (18000)\t\tin (1392)\n",
            "and (17474)\t\tand (1391)\n",
            "'s (9784)\t\t's (868)\n",
            "that (8931)\t\tfor (726)\n",
            "for (8927)\t\t$ (659)\n",
            "$ (7541)\t\tthat (657)\n",
            "is (7337)\t\tit (537)\n",
            "it (6112)\t\tis (529)\n",
            "said (6027)\t\tsaid (513)\n",
            "on (5650)\t\ton (486)\n",
            "by (4915)\t\tat (453)\n",
            "at (4894)\t\twas (436)\n",
            "as (4833)\t\tas (402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wCHz8MhRBvoQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given that the training text is much larger than the validation text, it is normal that the absolute frequencies in the training text are much larger. The ranking of the words is more interesting, and we see that even in the top 20, there are small differences. For the medium- and low-frequency ranges, the differences will become larger:"
      ]
    },
    {
      "metadata": {
        "id": "X0lwxGw9Ap_i",
        "colab_type": "code",
        "outputId": "9df2a1f0-0db5-4398-e0df-e2574ca258f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "cell_type": "code",
      "source": [
        "print('Train (freq.)\\t\\tValid (freq.)')\n",
        "for i in range(200,250):\n",
        "  print('{0} ({1})\\t\\t{2} ({3})'.format(count_pairs[i][0],count_pairs[i][1],count_pairs_valid[i][0],count_pairs_valid[i][1]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (freq.)\t\tValid (freq.)\n",
            "well (462)\t\tended (40)\n",
            "part (461)\t\trevenue (40)\n",
            "fell (459)\t\tsee (40)\n",
            "japan (459)\t\tseveral (40)\n",
            "another (457)\t\tdays (39)\n",
            "should (457)\t\tget (39)\n",
            "higher (453)\t\thigher (39)\n",
            "debt (452)\t\tincluding (39)\n",
            "offer (448)\t\tblack (38)\n",
            "take (448)\t\tclose (38)\n",
            "including (445)\t\tfirms (38)\n",
            "among (444)\t\tgeneral (38)\n",
            "court (444)\t\tissues (38)\n",
            "being (443)\t\twell (38)\n",
            "according (442)\t\taround (37)\n",
            "each (442)\t\tchicago (37)\n",
            "index (440)\t\tconcern (37)\n",
            "tax (437)\t\tdrop (37)\n",
            "trade (431)\t\thigh (37)\n",
            "world (431)\t\tmight (37)\n",
            "reported (430)\t\tpoint (37)\n",
            "work (426)\t\tsale (37)\n",
            "operations (424)\t\tsold (37)\n",
            "then (422)\t\tamerican (36)\n",
            "computer (420)\t\tamong (36)\n",
            "past (420)\t\tdecline (36)\n",
            "sale (419)\t\tfinancial (36)\n",
            "however (416)\t\tinternational (36)\n",
            "our (416)\t\tmanagement (36)\n",
            "way (416)\t\tmonday (36)\n",
            "lower (413)\t\tplunge (36)\n",
            "plans (412)\t\tshe (36)\n",
            "vice (412)\t\tsmall (36)\n",
            "economic (410)\t\tagreed (35)\n",
            "department (409)\t\tcapital (35)\n",
            "end (409)\t\tlate (35)\n",
            "yield (409)\t\tlosses (35)\n",
            "report (406)\t\tmade (35)\n",
            "sold (402)\t\tnext (35)\n",
            "insurance (401)\t\topened (35)\n",
            "growth (400)\t\twhere (35)\n",
            "high (400)\t\tboth (34)\n",
            "how (399)\t\tcontract (34)\n",
            "foreign (397)\t\tgovernment (34)\n",
            "increase (397)\t\tlower (34)\n",
            "less (396)\t\tproducts (34)\n",
            "common (395)\t\tset (34)\n",
            "banks (394)\t\tvery (34)\n",
            "closed (394)\t\tworld (34)\n",
            "several (394)\t\tcommon (33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5Fok7dfA6L9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now create a mapping from words to indices. The real input for the neural network will be indices, because they take up less space and because it makes certain operations easier."
      ]
    },
    {
      "metadata": {
        "id": "PGMZqJFVAfoh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# words = list of all the words (in decreasing frequency)\n",
        "items, _ = list(zip(*count_pairs))\n",
        "\n",
        "# make a dictionary with a mapping from each word to an id; word with highest frequency gets lowest id etc.\n",
        "item_to_id = dict(zip(items, range(len(items))))\n",
        "id_to_item = dict(zip(range(len(items)), items))\n",
        "vocab_size = len(item_to_id)\n",
        "\n",
        "# convert the words to indices\n",
        "train_ids_large = [item_to_id[item] for item in train_text]\n",
        "valid_ids_large = [item_to_id[item] for item in valid_text]\n",
        "test_ids_large = [item_to_id[item] for item in test_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6KQShlZFcGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the data is converted to ids, it looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "Vlo5SpAHFhLq",
        "colab_type": "code",
        "outputId": "33e84891-01d2-4c24-9c41-23dd32dd6ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        }
      },
      "cell_type": "code",
      "source": [
        "print('Here is an example of words and their indices:')\n",
        "for i in range(40):\n",
        "  print('{0}\\t{1}'.format(valid_text[i], valid_ids_large[i]))\n",
        "print('\\nAnd this is wat the input looks like, a list of indices:')\n",
        "print(valid_ids_large[:40])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is an example of words and their indices:\n",
            "consumers\t1132\n",
            "may\t93\n",
            "want\t358\n",
            "to\t5\n",
            "move\t329\n",
            "their\t51\n",
            "telephones\t9836\n",
            "a\t6\n",
            "little\t326\n",
            "closer\t2476\n",
            "to\t5\n",
            "the\t0\n",
            "tv\t662\n",
            "set\t388\n",
            "<eos>\t2\n",
            "<unk>\t1\n",
            "<unk>\t1\n",
            "watching\t2974\n",
            "abc\t2158\n",
            "'s\t9\n",
            "monday\t381\n",
            "night\t1068\n",
            "football\t2347\n",
            "can\t89\n",
            "now\t99\n",
            "vote\t847\n",
            "during\t198\n",
            "<unk>\t1\n",
            "for\t11\n",
            "the\t0\n",
            "greatest\t3383\n",
            "play\t1119\n",
            "in\t7\n",
            "N\t3\n",
            "years\t72\n",
            "from\t20\n",
            "among\t211\n",
            "four\t346\n",
            "or\t36\n",
            "five\t258\n",
            "\n",
            "And this is wat the input looks like, a list of indices:\n",
            "[1132, 93, 358, 5, 329, 51, 9836, 6, 326, 2476, 5, 0, 662, 388, 2, 1, 1, 2974, 2158, 9, 381, 1068, 2347, 89, 99, 847, 198, 1, 11, 0, 3383, 1119, 7, 3, 72, 20, 211, 346, 36, 258]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MR026sYjd3sO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To speed up some experiments, we take a subset of the data."
      ]
    },
    {
      "metadata": {
        "id": "kom3XmMHdxQt",
        "colab_type": "code",
        "outputId": "281d61ac-9092-4cbc-9866-dfa013d99b42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "# take a smaller subset to speed up training\n",
        "train_ids = train_ids_large[:50000]\n",
        "valid_ids = valid_ids_large[:10000]\n",
        "test_ids = test_ids_large[:10000]\n",
        "\n",
        "print('Number of words in small training set: {0}'.format(len(train_ids)))\n",
        "print('Number of words in small validation set: {0}'.format(len(valid_ids)))\n",
        "print('Number of words in small test set: {0}'.format(len(test_ids)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in small training set: 50000\n",
            "Number of words in small validation set: 10000\n",
            "Number of words in small test set: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIhqMJ2CKMxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The classes and function that we will use"
      ]
    },
    {
      "metadata": {
        "id": "TuNJPpA-oZRs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now define the classes and functions that we will use for training and testing our language models."
      ]
    },
    {
      "metadata": {
        "id": "jBnJVfPuNzmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The class for an RNN language model is **rnn_lm.rnn_lm()**. We will see later which options we can use."
      ]
    },
    {
      "metadata": {
        "id": "tIHeGqTCW8J-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**batchGenerator.batchGenerator(<dataset\\>)** is class that will generate mini-batches from the data. <dataset\\> is a list of word ids."
      ]
    },
    {
      "metadata": {
        "id": "z_SB0UTRdnU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "batchGenerator is a class that will iterate over the data set and create **mini-batches** that will be the input for the neural network. A mini-batch contains several sentences/word sequences, and feeding mini-batches instead of a single sentence or a single word to the network speeds up the processing, and also causes better convergence of the model.\n",
        "\n",
        "The batches are matrices of the size **batch_size* x **num_steps**. Batch_size is the number of different sequences in a single batch, and num_steps the length of each  sequence.\n",
        "\n",
        "Here is an example of how batchGenerator can be used. You will notice that the target batch contains the same indices as the input batch, but shifted one (time) step to the right."
      ]
    },
    {
      "metadata": {
        "id": "qHiY_eJ0dpUa",
        "colab_type": "code",
        "outputId": "0d6fd481-1fff-4ca6-ffc3-b040307e5d48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_steps = 50\n",
        "\n",
        "generator = batchGenerator(valid_ids, batch_size=batch_size, num_steps=num_steps)\n",
        "input_batch, target_batch, end_reached = generator.generate()\n",
        "print('Shape of the mini-batch: {0}'.format(input_batch.shape))\n",
        "print('This is what an input batch looks like:\\n{0}'.format(input_batch))\n",
        "print('And this is what a target batch looks like:\\n{0}'.format(target_batch))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the mini-batch: (32, 50)\n",
            "This is what an input batch looks like:\n",
            "[[1132   93  358 ...    4  249 1795]\n",
            " [   4    3 3770 ...    2    0  361]\n",
            " [ 967   33   25 ...  769 2737    2]\n",
            " ...\n",
            " [  12    3   48 ... 1470    2   54]\n",
            " [ 505    7    1 ...  660   43  299]\n",
            " [   1 2034    8 ...   11   99   29]]\n",
            "And this is what a target batch looks like:\n",
            "[[  93  358    5 ...  249 1795    1]\n",
            " [   3 3770 1619 ...    0  361    4]\n",
            " [  33   25 2047 ... 2737    2 2158]\n",
            " ...\n",
            " [   3   48    7 ...    2   54 1068]\n",
            " [   7    1   50 ...   43  299 9642]\n",
            " [2034    8  377 ...   99   29   28]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ibvCjKOJI9h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a function which pretty-prints what the mini-batches look like. You can give if a batch as first argument, and the index that you want to look at. In our case, there are 32 sequences in every min-batch, so the indices range between 0 and 31 (in Python, indices always start at 0)."
      ]
    },
    {
      "metadata": {
        "id": "-hMxzS8dHC-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_batch(batch, idx):\n",
        "  for i in range(num_steps):\n",
        "      word = id_to_item[batch[idx][i]]\n",
        "      if word == '<eos>':\n",
        "         print()\n",
        "      else:\n",
        "        print(word, end=' ')\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q0RGWHE1Jnvv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And here are some examples of what the first and fourth sequence of the  input and target batch look like. Try it yourself with some new values."
      ]
    },
    {
      "metadata": {
        "id": "oG3sipRFJh7H",
        "colab_type": "code",
        "outputId": "856a6824-ac0c-4009-ddbd-f9303e504b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "cell_type": "code",
      "source": [
        "print_batch(input_batch, 0)\n",
        "print_batch(target_batch, 0)\n",
        "\n",
        "print_batch(input_batch, 3)\n",
        "print_batch(target_batch, 3)\n",
        "\n",
        "# try it yourself:\n",
        "# print_batch(..., ...)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "consumers may want to move their telephones a little closer to the tv set \n",
            "<unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            "two weeks ago viewers of several nbc \n",
            "\n",
            "may want to move their telephones a little closer to the tv set \n",
            "<unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            "two weeks ago viewers of several nbc <unk> \n",
            "\n",
            "says nbc has been able to charge premium rates for this ad time \n",
            "she would n't say what the premium is but it 's believed to be about N N above regular <unk> rates \n",
            "we were able to get advertisers to use their promotion budget for this because \n",
            "\n",
            "nbc has been able to charge premium rates for this ad time \n",
            "she would n't say what the premium is but it 's believed to be about N N above regular <unk> rates \n",
            "we were able to get advertisers to use their promotion budget for this because they \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CY4KxCsUenOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In run_lm, there are two functions that can be used to train and/or test a model. \n",
        "\n",
        "**run_lm.run_lm():** this function can be called to build, train and test models with different parameter settings. "
      ]
    },
    {
      "metadata": {
        "id": "vBbmu8MyeRjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**run_lm.run_epoch()**: this is a function that does one pass over the whole dataset. If we are training the model, it will update the parameters and return the perplexity. Otherwise, it will just return the perplexity."
      ]
    },
    {
      "metadata": {
        "id": "UozYDTgTPYBN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "DIQhiqR2joDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Start by running the cell below, which will load a large matrix. During loading, which will take a few minutes, you can continue to read the explanation below. "
      ]
    },
    {
      "metadata": {
        "id": "1PSMzMojSBb2",
        "colab_type": "code",
        "outputId": "49372c3f-07a2-4bd1-b97c-bda35f5e5624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "url_emb_matrix = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/models/emb_matrix_ptb.txt'\n",
        "emb_matrix = np.loadtxt(urllib.urlopen(url_emb_matrix))\n",
        "\n",
        "print('Size of the embedding matrix: {0}'.format(emb_matrix.shape))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the embedding matrix: (10001, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yUR-tGjo7x-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Often the input words for a language model are represented as indices in a vocabulary, or **one-hot vectors** (where all values are 0 except the index of the word, which has value 1). This representation is a discrete representation, just like in n-gram language models. It has the disadvantage that relationships between words (e.g. the syntactic relationship between 'eat' and 'eating', or the semantic relationship between 'eat' and 'drink') can not be inferred from the word representations. "
      ]
    },
    {
      "metadata": {
        "id": "QNYBzUniRG1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural language models however, do not use this representation as is but first map it to a continuous, lower-dimensional vector, also called **word embedding**. They do this by looking up the index of the word in a weight matrix $\\mathbf{E}$, which is often called the embedding matrix. By training the embedding matrix jointly with the rest of the language model, the resulting word embeddings will have some interesting properties: several syntactic and semantic relationships are encoded as vector offsets in the embedding space. A famous example is the vector offset for male - female, which is shown in the example below:"
      ]
    },
    {
      "metadata": {
        "id": "M3DDEbxR7aka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/lverwimp/RNN_language_modeling/blob/master/kingqueen.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "PRSLTNH9kC4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The embedding matrix that we loaded above contains the embeddings of a large language model trained on Penn TreeBank. We will not train such a large model because the training time is too long (ca. 1h, depending on the hardware). In the next chapter, we will train smaller models on a subset of Penn TreeBank, which can be trained in a couple minutes."
      ]
    },
    {
      "metadata": {
        "id": "ZowKb-VyqZp2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now take a look at what the embeddings look like. Simple looking at their values does not tell us much:"
      ]
    },
    {
      "metadata": {
        "id": "DUt7sr47qk3H",
        "colab_type": "code",
        "outputId": "a0273197-7a5d-4c90-b8ca-46360356ffd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4463
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"This is what the embedding for 'man' looks like: {0}\".format(emb_matrix[item_to_id['man']]))\n",
        "print(\"This is what the embedding for 'woman' looks like: {0}\".format(emb_matrix[item_to_id['woman']]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is what the embedding for 'man' looks like: [ 2.22551916e-02 -9.13960487e-03 -9.96953025e-02  1.44256707e-02\n",
            " -8.93871710e-02 -5.56497090e-02 -2.88526248e-02 -1.64089501e-01\n",
            "  5.57105020e-02  1.36711285e-01  1.70425534e-01 -5.21288551e-02\n",
            " -1.39592379e-01  2.38559663e-01  1.06275730e-01 -9.09598172e-02\n",
            " -1.60241395e-01  3.05982172e-01  2.08467975e-01  7.41385967e-02\n",
            " -2.93741673e-01 -6.73426390e-02  1.00698069e-01 -1.55318335e-01\n",
            "  2.01973036e-01  4.75828983e-02  9.14453994e-03  3.46774571e-02\n",
            "  1.36753276e-01  1.79032646e-02 -6.12525560e-04  2.35116452e-01\n",
            " -2.99999444e-03 -4.93851304e-03  8.87784641e-03 -3.33922133e-02\n",
            " -1.55926179e-02  4.35285084e-02 -2.02515557e-01 -1.23816490e-01\n",
            " -1.23949915e-01 -1.11349218e-01 -1.21268012e-01 -2.02699125e-01\n",
            "  9.83564705e-02 -1.28600216e-02 -5.45965955e-02 -1.67813286e-01\n",
            " -1.71364948e-01  1.11058034e-01 -2.72298574e-01  4.57900465e-02\n",
            " -8.33498780e-03 -1.49039909e-01  1.39850318e-01  8.90836641e-02\n",
            " -7.58693144e-02  5.85546419e-02 -1.55996541e-02  3.21961790e-02\n",
            "  1.49037495e-01 -1.28655270e-01 -2.09699228e-01  4.60150652e-02\n",
            "  1.10365404e-02 -3.63006294e-01  3.55068818e-02 -6.00833632e-02\n",
            "  1.26082033e-01 -1.34636924e-01  1.21092029e-01  4.66313958e-02\n",
            "  2.18401879e-01  4.97232042e-02 -5.06668426e-02 -2.06769002e-03\n",
            "  5.83822876e-02  1.31914794e-01 -3.26612949e-01  1.89276859e-01\n",
            "  1.03747216e-03 -2.07834721e-01 -2.09646419e-01  1.25808761e-01\n",
            " -3.40957046e-02  9.49826017e-02  1.23019703e-01 -1.97895512e-01\n",
            "  1.82085857e-01  2.63438188e-02 -8.55142996e-03 -3.28684524e-02\n",
            " -3.59613448e-02 -7.08581284e-02 -2.85420239e-01  4.26912010e-01\n",
            "  1.38946563e-01  1.37316033e-01 -1.38727739e-01  2.25996263e-02\n",
            " -8.31313282e-02 -5.65744564e-02 -1.47672728e-01 -3.16622034e-02\n",
            " -1.38064459e-01  1.48869708e-01  1.39266312e-01  2.74830684e-02\n",
            " -3.03424075e-02  7.87170231e-02  1.78105697e-01  7.64603121e-03\n",
            " -4.51739803e-02  1.46205813e-01  1.10155307e-01 -1.19822398e-01\n",
            " -2.42335692e-01  2.52487004e-01 -9.48293805e-02  2.37186655e-01\n",
            " -7.60126784e-02 -1.52584523e-01 -6.17287681e-02 -1.70994475e-02\n",
            " -1.62325889e-01  2.55216688e-01  2.82934576e-01  2.14019433e-01\n",
            "  1.46343410e-01  4.26797062e-01 -1.57262340e-01 -1.89740598e-01\n",
            " -1.80498540e-01  5.33368252e-02  1.19260073e-01 -1.98133349e-01\n",
            "  1.90068468e-01  3.27925310e-02  1.00603633e-01  1.41027287e-01\n",
            "  9.63953510e-03  9.67168137e-02  8.45652912e-03 -3.56586576e-02\n",
            " -1.05841756e-02 -1.26230016e-01  1.92603208e-02  4.09035496e-02\n",
            " -1.17723949e-01  1.13221988e-01  2.62949497e-01 -9.40651521e-02\n",
            " -1.00377522e-01 -2.26265728e-01 -9.94822681e-02  2.18836829e-01\n",
            " -1.64124802e-01  4.15628739e-02  1.18451476e-01  2.68450201e-01\n",
            "  2.17113998e-02 -1.95980415e-01  1.90791339e-01 -5.24763390e-02\n",
            "  5.68223521e-02  2.21552745e-01 -4.81095351e-02 -3.01960367e-03\n",
            " -2.46087164e-02 -3.00537527e-01  1.61866546e-01  8.82238597e-02\n",
            "  3.49293873e-02  3.37342501e-01  1.61123469e-01  2.09764540e-01\n",
            "  2.55354077e-01 -1.28009513e-01 -1.25212923e-01  6.40658140e-02\n",
            "  6.26614317e-02 -9.42884907e-02  2.38281097e-02  2.13632897e-01\n",
            " -3.37940082e-02  1.35756642e-01  1.94136739e-01  9.05532166e-02\n",
            " -6.67104796e-02  2.43541315e-01  2.04080623e-02 -8.33504796e-02\n",
            "  3.65471661e-01  1.73911713e-02 -2.13010479e-02  6.75384477e-02\n",
            "  1.45945609e-01 -6.04129881e-02 -2.36964136e-01  2.98889875e-01\n",
            "  1.05539419e-01 -8.84779170e-02  2.31032789e-01  2.62228161e-01\n",
            "  1.01086773e-01 -1.44373059e-01  8.64883065e-02  3.44718397e-02\n",
            "  3.27519327e-01  5.00034578e-02 -8.04103613e-02 -1.21065356e-01\n",
            "  1.10282071e-01 -2.32044175e-01  6.14852794e-02  1.24860168e-01\n",
            "  1.48938179e-01  1.43781334e-01 -8.67105424e-02 -2.30098337e-01\n",
            "  2.16625948e-02 -1.49460658e-01  1.32610127e-01 -2.04913154e-01\n",
            " -1.45035803e-01  3.35195512e-02  6.31300965e-03 -1.20207600e-01\n",
            " -2.03093126e-01 -1.69105560e-01  1.16067119e-01  1.55451983e-01\n",
            " -4.15156223e-02 -2.56269965e-02 -5.08730039e-02  1.63907290e-01\n",
            " -1.20169669e-01  6.74193650e-02  1.11591980e-01  3.37681174e-02\n",
            " -1.87900476e-02  9.61455554e-02  8.06842372e-02  6.51066527e-02\n",
            "  3.08724374e-01  6.00484535e-02  1.71754405e-01  5.33208475e-02\n",
            " -3.50281715e-01  1.44232512e-01  8.97235125e-02 -2.06487253e-02\n",
            " -9.94816888e-03 -1.29824176e-01  6.79122135e-02 -1.55616075e-01\n",
            "  5.90303876e-02  1.07809687e-02 -2.02542748e-02 -5.80157042e-02\n",
            " -1.56292230e-01  3.41894865e-01  8.33201036e-02 -1.70205712e-01\n",
            "  4.54839058e-02 -8.54450092e-02 -1.62276160e-02 -1.94099277e-01\n",
            " -6.27452955e-02 -2.16369241e-01 -7.72863179e-02 -3.06398451e-01\n",
            "  3.71957481e-01 -1.62292093e-01 -2.08881512e-01 -7.04039112e-02\n",
            "  7.50898123e-02 -1.05852462e-01  2.58691698e-01  8.44491348e-02\n",
            "  6.22642785e-02 -1.30486280e-01 -1.22932471e-01 -2.34928921e-01\n",
            " -3.28927368e-01  2.16643572e-01  1.15665488e-01  3.41943093e-02\n",
            " -3.02431405e-01  2.52162479e-02  5.10152616e-02  2.71435916e-01\n",
            "  1.49760172e-01 -4.78087291e-02  1.94306821e-01 -2.38290191e-01\n",
            " -1.26796886e-01 -5.50160743e-02 -5.13103753e-02 -2.46120080e-01\n",
            " -5.71428835e-02 -1.38884157e-01 -3.91057953e-02 -1.24645151e-01\n",
            "  3.94316018e-02  6.38034865e-02  6.26248792e-02  1.52108103e-01\n",
            " -5.94460778e-03  9.44877565e-02 -1.78429633e-01 -2.85029095e-02\n",
            " -2.73032300e-02 -1.53392315e-01 -1.30156279e-01  8.06939751e-02\n",
            " -2.85861287e-02 -3.06206457e-02 -1.63662151e-01  5.76925837e-02\n",
            "  2.61521954e-02  6.68574721e-02  7.83969834e-02  1.73907533e-01\n",
            " -1.50179580e-01  1.29038244e-01  3.45160931e-01 -2.19987303e-01\n",
            "  1.83463916e-01  1.66245908e-01 -1.54295757e-01  2.00909190e-02\n",
            "  1.16813481e-01 -3.15155774e-01  2.24867482e-02 -5.47650494e-02\n",
            "  2.08488569e-01  2.73680866e-01  1.52271941e-01 -2.31156461e-02\n",
            " -7.55682811e-02  1.79648787e-01  8.70141834e-02 -4.67264745e-03\n",
            " -7.59888021e-03  2.89434474e-03  3.48315597e-01  2.75734395e-01\n",
            "  4.68306959e-01  1.94712862e-01 -5.09172911e-03 -5.18523864e-02\n",
            " -9.39932317e-02  1.84923189e-03  2.47957781e-01  1.37279645e-01\n",
            " -2.63749696e-02  1.78477615e-01  1.14164144e-01  1.32988796e-01\n",
            "  1.05961688e-01 -4.82306257e-02  1.75587609e-02 -6.72257617e-02\n",
            "  1.95508927e-01  2.21572638e-01 -3.96757200e-03 -1.07902423e-01\n",
            " -9.54711437e-02 -9.23533812e-02 -5.84414750e-02 -1.12836868e-01\n",
            "  1.77445952e-02 -3.18487734e-02  2.69419402e-02 -8.63948911e-02\n",
            " -1.13106556e-01 -4.14304957e-02 -2.03622565e-01  1.36392772e-01\n",
            " -2.58261234e-01 -9.33378562e-02  5.58584072e-02 -2.07945988e-01\n",
            "  1.45121202e-01 -8.02498162e-02  1.88014582e-02 -2.10657082e-02\n",
            " -1.06315285e-01 -3.78117673e-02 -5.21151908e-02 -1.48252159e-01\n",
            "  3.13419223e-01  9.96029750e-02  5.08262515e-02 -1.16659120e-01\n",
            " -8.28932822e-02  3.54289572e-04  2.80714082e-03 -1.73818901e-01\n",
            "  4.69626822e-02 -1.67657867e-01  9.15219337e-02 -1.10350139e-01\n",
            "  3.59775648e-02  2.08763052e-02 -1.17831387e-01 -1.13238417e-01\n",
            " -1.86729193e-01 -2.95383856e-02 -4.99341562e-02 -3.09505105e-01\n",
            " -2.82572284e-02 -4.20782305e-02 -2.17958406e-01 -3.01156044e-01\n",
            "  6.58878908e-02  1.45670809e-02  1.48143128e-01 -2.45801702e-01\n",
            " -1.49195910e-01  1.48955822e-01 -3.81399281e-02  7.06722066e-02\n",
            " -3.09837222e-01  1.67221464e-02 -1.51504442e-01  2.22403273e-01\n",
            "  8.16400498e-02  1.17495589e-01  1.66024925e-04  7.82753080e-02\n",
            "  1.17658861e-01  1.05326772e-02 -1.26420274e-01  3.53396051e-02\n",
            "  1.82805330e-01  1.33595290e-02  1.18519925e-01 -1.29518524e-01\n",
            " -1.16184525e-01  2.20957160e-01  2.50120580e-01  2.73072004e-01\n",
            " -8.84649083e-02 -7.81552047e-02  1.63194567e-01  1.44454226e-01\n",
            " -7.82532468e-02  8.13282281e-02  1.29323721e-01 -1.45089507e-01\n",
            "  3.48938555e-02 -1.43165700e-04  1.55088454e-01 -3.43190521e-01\n",
            " -1.58649944e-02 -1.03718422e-01  1.11841731e-01  9.87114906e-02\n",
            "  2.26683274e-01 -1.78738937e-01 -9.61368307e-02 -1.18308991e-01\n",
            " -2.91287899e-01 -1.06734425e-01  2.83757430e-02 -7.14757890e-02\n",
            " -1.79196879e-01  1.54962569e-01  4.32617441e-02 -4.48118784e-02\n",
            "  3.82394373e-01 -3.60894576e-02 -1.03694372e-01 -1.15135901e-01\n",
            "  1.43103793e-01 -2.57355031e-02 -1.13199636e-01  9.83266067e-03\n",
            "  6.08845316e-02 -1.03322275e-01 -1.49380630e-02 -2.57032275e-01\n",
            "  5.79671422e-03 -3.97462286e-02  3.08103822e-02  2.51326859e-02\n",
            "  3.99231493e-01 -3.12863529e-01 -7.87647441e-02 -9.21510532e-02\n",
            "  7.01465234e-02 -2.87634414e-02  1.48845436e-02  8.32773224e-02\n",
            "  9.55533324e-05 -1.03755854e-01 -1.82664245e-01 -1.33138373e-01\n",
            " -7.28684515e-02 -6.62487671e-02  5.12671649e-01 -1.74841844e-02\n",
            "  1.58413604e-01 -2.30647713e-01  3.79327908e-02  1.75432730e-02\n",
            " -3.14449589e-03  2.42176086e-01  1.99566752e-01 -7.43980240e-03]\n",
            "This is what the embedding for 'woman' looks like: [ 1.74308002e-01 -7.32144639e-02 -1.02590239e-02 -1.88533872e-01\n",
            " -2.09980443e-01  9.95805487e-02  8.78231227e-02  5.85651137e-02\n",
            "  3.66241261e-02  7.08711147e-02  2.82564983e-02 -1.07463188e-01\n",
            " -1.27332747e-01  1.11715145e-01 -2.05866039e-01 -1.12852015e-01\n",
            "  1.94973335e-01  1.45556375e-01  5.11207804e-02  1.52785897e-01\n",
            "  1.34671688e-01 -2.05109060e-01 -1.02570437e-01 -6.68717697e-02\n",
            "  1.45583913e-01  4.00012881e-02  3.29656061e-03  1.31140813e-01\n",
            "  1.24032632e-01 -3.61399837e-02 -1.21571042e-01  3.55840027e-02\n",
            "  1.15449280e-01  9.60956588e-02 -2.15111300e-02  2.70436168e-01\n",
            " -3.78862694e-02  7.77770132e-02 -4.38121557e-02 -7.41952956e-02\n",
            "  9.64568257e-02  5.81153035e-02 -7.69275725e-02  2.24479035e-01\n",
            " -5.25207259e-02 -1.24683371e-02 -1.13851719e-01  6.12013228e-02\n",
            " -9.47978348e-04 -4.26200218e-02 -2.24259585e-01 -6.90839486e-03\n",
            " -9.86377746e-02  9.77145880e-02  1.29887074e-01  2.60102808e-01\n",
            "  9.64476690e-02  9.91566181e-02  3.47931241e-03  1.48139432e-01\n",
            "  5.47672249e-02 -2.67782092e-01 -2.90023267e-01 -1.15457788e-01\n",
            " -1.41968513e-02 -2.09729552e-01  2.67756134e-01  1.57625496e-01\n",
            " -1.15358226e-01 -2.44086951e-01  8.44427347e-02 -1.24627493e-01\n",
            "  6.05642758e-02  2.34205306e-01 -6.17263429e-02 -1.31007042e-02\n",
            " -2.36306842e-02  1.12938531e-01 -8.89128819e-02  2.69634631e-02\n",
            "  1.96234688e-01 -1.46121010e-02 -2.97513366e-01  2.09814832e-01\n",
            "  2.46878639e-01 -1.63339339e-02  3.20260637e-02 -1.09195992e-01\n",
            "  2.31520310e-01 -8.94171521e-02 -7.59601444e-02  8.14308524e-02\n",
            "  1.83150731e-02  4.99666557e-02  1.99858416e-02  3.10814857e-01\n",
            "  6.74109459e-02  1.75076053e-01 -1.18478887e-01  1.62017107e-01\n",
            "  2.58456469e-02 -3.48015316e-02 -1.82364151e-01 -7.28103817e-02\n",
            " -3.45771462e-01 -1.06386445e-01 -1.21377856e-01  1.98242106e-02\n",
            "  3.25331129e-02 -1.69834588e-02  3.25240344e-01 -1.56507343e-02\n",
            "  1.03665525e-02  1.35685429e-01  2.39107415e-01 -9.71117020e-02\n",
            "  6.84719086e-02 -7.88921937e-02  1.17052287e-01  2.02540159e-01\n",
            "  8.30946937e-02 -4.27745543e-02 -1.06247410e-01  5.66605963e-02\n",
            " -1.74197122e-01 -6.51034527e-03  2.29043901e-01  4.35913056e-02\n",
            "  1.14857696e-01 -6.85909949e-03 -1.21325463e-01 -5.86162508e-02\n",
            " -1.55001074e-01 -2.22731084e-02  1.56435043e-01  6.76601082e-02\n",
            " -1.17249049e-01 -1.41687512e-01 -2.05110073e-01  9.07546431e-02\n",
            "  6.53837472e-02 -3.98570411e-02  1.73359700e-02  6.18780218e-02\n",
            " -1.59342840e-01  7.79064000e-02  1.02607667e-01 -7.67515525e-02\n",
            "  1.86304465e-01 -6.07027337e-02  1.46954253e-01 -9.14494321e-02\n",
            " -2.07901627e-01 -1.54264160e-02 -2.58239776e-01 -5.47538660e-02\n",
            " -9.34381597e-03  1.50432721e-01  2.68404968e-02  2.03806877e-01\n",
            " -2.11120725e-01  8.92522465e-03 -2.05072351e-02 -9.04520079e-02\n",
            "  1.22676201e-01 -2.30646748e-02 -8.70473590e-03 -1.73461214e-02\n",
            "  1.45244617e-02 -1.72766730e-01  2.23063864e-02 -1.07191376e-01\n",
            "  6.74993768e-02  7.10930154e-02 -9.12552550e-02  2.86836177e-02\n",
            "  2.17798546e-01 -2.22206071e-01  1.35353148e-01  9.75046679e-02\n",
            "  1.07550673e-01 -1.99087054e-01  4.77801561e-02  6.25923276e-02\n",
            " -5.33675440e-02 -6.04246669e-02  1.37612551e-01 -2.89794113e-02\n",
            "  1.42849550e-01  1.06801845e-01 -1.14978544e-01  2.31773537e-02\n",
            "  1.46927893e-01  8.59171078e-02  1.49662226e-01  9.84301046e-03\n",
            "  5.71276955e-02 -2.52328515e-01  5.78376949e-02 -3.26684356e-01\n",
            " -1.81210369e-01  1.89328849e-01  6.60918802e-02 -2.36547530e-01\n",
            "  2.22030953e-02  2.40963310e-01  1.61554396e-01 -1.53852567e-01\n",
            "  2.54917949e-01  4.81277555e-02 -5.36896139e-02 -2.93632057e-02\n",
            " -2.04353537e-02  1.30428672e-01  2.91455358e-01  9.28239897e-02\n",
            "  6.81672916e-02  2.14011535e-01  6.10989593e-02  2.28472665e-01\n",
            "  1.00446321e-01 -3.93539190e-01  5.03938720e-02  4.64284457e-02\n",
            "  1.24245897e-01  2.17940882e-01 -2.49650013e-02 -8.60415250e-02\n",
            " -1.20868338e-02  4.48107831e-02  7.23054558e-02 -1.09980740e-01\n",
            " -6.54088380e-03  1.54140383e-01 -2.84594595e-01  4.15679514e-01\n",
            "  2.71909256e-02  3.62182297e-02  1.73226446e-01  3.00642729e-01\n",
            " -1.12947620e-01  7.12687373e-02 -1.65543392e-01  6.36766255e-02\n",
            " -8.40659887e-02 -1.63427386e-02  1.03963055e-01  5.82237281e-02\n",
            " -2.60171264e-01  3.32544029e-01 -1.61814943e-01 -1.30823791e-01\n",
            "  3.29728350e-02 -7.45608136e-02 -3.94125618e-02  6.00765683e-02\n",
            " -5.38796484e-02  7.56588429e-02 -2.26537928e-01 -9.99501348e-02\n",
            " -4.26271446e-02 -1.62520021e-01  1.12994328e-01 -1.38300627e-01\n",
            "  9.54894498e-02  1.07283756e-01 -3.17522734e-02 -8.70925002e-03\n",
            " -8.66551474e-02 -1.58602163e-01  3.31274457e-02 -9.55147520e-02\n",
            "  1.24321587e-01 -7.56525248e-02 -6.54705390e-02  1.43169835e-02\n",
            "  2.31263562e-04 -9.23176855e-02  1.55826718e-01  1.98476255e-01\n",
            " -6.29099309e-02  1.33847326e-01  6.73631951e-02 -4.94706072e-02\n",
            " -5.29381752e-01  1.84391484e-01  1.71180844e-01  2.91043401e-01\n",
            " -3.44750822e-01 -7.74661601e-02  8.19205716e-02  1.22176297e-01\n",
            "  2.83677757e-01 -6.02470785e-02  1.65895551e-01 -1.49933711e-01\n",
            " -1.03100352e-01  1.01115823e-01 -2.24292934e-01 -2.33753044e-02\n",
            " -5.12097590e-03 -4.78171632e-02  4.60093580e-02 -1.18492112e-01\n",
            " -2.92245984e-01 -2.32822560e-02  2.98196763e-01 -1.02628291e-01\n",
            " -9.58825126e-02 -1.54428557e-01 -2.95334697e-01  8.66016224e-02\n",
            " -1.78329796e-01 -7.61601850e-02 -3.99478041e-02  3.74844261e-02\n",
            "  1.65563315e-01 -3.44403051e-02 -2.95651436e-01 -1.24331199e-01\n",
            "  1.21917315e-01  2.68059894e-02  1.73885778e-01  1.62837014e-01\n",
            "  6.93828613e-02  5.59955835e-03 -6.87891692e-02 -2.45949581e-01\n",
            " -1.30251288e-01 -1.48472637e-02 -3.50420952e-01  1.94120465e-03\n",
            "  2.71305386e-02  2.09606420e-02  1.45611674e-01 -1.71532333e-01\n",
            " -5.53883649e-02  1.01204686e-01  3.12528387e-02 -9.40993354e-02\n",
            "  1.30357414e-01 -4.91539314e-02  1.66977942e-01  5.17773218e-02\n",
            "  3.60412113e-02  5.87552004e-02  4.61896397e-02  8.60540792e-02\n",
            "  2.07728609e-01  1.38402626e-01 -1.50791034e-01 -4.47895229e-02\n",
            "  2.99610458e-02  9.17316694e-03  1.30084410e-01  6.44365475e-02\n",
            " -2.44603455e-01  1.60179734e-01 -9.16936174e-02 -1.54519621e-02\n",
            "  2.35074520e-01 -1.00435227e-01 -2.72086021e-02 -2.12632105e-01\n",
            "  2.62451708e-01 -1.59251660e-01 -4.15377021e-02  5.14412625e-03\n",
            " -2.23072901e-01  6.58183545e-03 -2.22153708e-01  8.45115483e-02\n",
            " -1.03373967e-01 -1.29141852e-01  4.46557164e-01 -8.49978924e-02\n",
            " -2.29920372e-01  1.02032386e-01 -1.18278533e-01  9.77101177e-02\n",
            " -1.19180612e-01 -1.75940290e-01  1.33774102e-01 -1.54656842e-01\n",
            " -5.58116182e-04  1.18548110e-01 -7.08832592e-02 -8.54906961e-02\n",
            "  1.24733355e-02 -4.89916056e-02 -1.23667538e-01 -1.35003878e-02\n",
            " -8.49698037e-02 -6.46174252e-02  1.77521318e-01 -3.10130209e-01\n",
            "  2.85580605e-01 -2.22597897e-01  6.13519624e-02 -7.00925887e-02\n",
            " -1.39348015e-01 -2.74462849e-02 -1.53159678e-01  4.68075722e-02\n",
            " -1.81635678e-01 -1.46948412e-01 -2.39286246e-03 -1.15918629e-01\n",
            " -3.11667860e-01 -1.96974844e-01 -2.24152192e-01  8.98396373e-02\n",
            "  6.16905056e-02 -9.20086280e-02 -2.22921789e-01  1.58664078e-01\n",
            "  5.63463010e-03  6.05549067e-02 -1.38124123e-01  3.55631635e-02\n",
            " -1.96035713e-01  2.17829257e-01 -7.42478180e-04 -2.25054063e-02\n",
            " -2.53578514e-01 -4.38647009e-02  4.38397415e-02  2.07221031e-01\n",
            " -2.49798432e-01  4.89957593e-02  8.26285034e-02 -1.84825584e-01\n",
            "  1.81640778e-02  6.89027831e-02 -1.97873652e-01 -7.61347339e-02\n",
            " -1.52147204e-01  1.23397201e-01 -1.88594759e-01 -1.50896460e-01\n",
            " -2.73138255e-01 -2.08822489e-02 -8.61065835e-02  2.55334917e-02\n",
            " -5.26633263e-01 -4.95675281e-02  8.65430012e-02  2.92994380e-01\n",
            "  5.66793084e-02 -3.44987623e-02  1.01840891e-01 -2.57307559e-01\n",
            " -1.32840380e-01  1.01382405e-01  1.46133363e-01  2.92446129e-02\n",
            "  1.41302735e-01 -1.95171721e-02 -1.74895048e-01  8.75800326e-02\n",
            "  8.69121403e-02 -1.16913229e-01  1.75265536e-01  1.68421432e-01\n",
            " -1.22678176e-01 -2.75343269e-01  1.70025185e-01 -2.04962157e-02\n",
            " -3.06968808e-01 -9.63657424e-02  5.86195216e-02  3.44155617e-02\n",
            " -1.61223505e-02  1.59189343e-01  9.56994444e-02  8.40338618e-02\n",
            " -1.99293755e-02 -8.26414153e-02  3.02488599e-02  1.06217684e-02\n",
            "  5.34731485e-02  1.31430611e-01 -7.33166886e-03  9.64256525e-02\n",
            "  2.88839281e-01 -9.90633816e-02  2.88899485e-02  1.49506286e-01\n",
            "  2.09690422e-01 -1.60925224e-01  8.58736187e-02  2.67330301e-03\n",
            " -6.58378899e-02 -1.93853546e-02 -3.84968221e-02  1.75656781e-01\n",
            " -1.93084236e-02  5.94887212e-02 -3.18226039e-01 -4.33385745e-02\n",
            " -3.54611985e-02  4.71592210e-02  9.66317356e-02  1.25632957e-01\n",
            "  9.27970335e-02 -2.96146333e-01  4.10154350e-02  1.00825140e-02\n",
            "  2.07098767e-01 -3.34036350e-02  1.39197931e-01 -4.71712500e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "auWYJWXdrlLU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Try it yourself for another word:"
      ]
    },
    {
      "metadata": {
        "id": "osPB09mvrJcy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# if you want you can look at the embedding values of another word you're interesting in\n",
        "# simply change 'woman' in the print statement below\n",
        "# !!! note: there will be a KeyError if you try a word that is not in the vocabulary\n",
        "# print(\"This is what the embedding for 'woman' looks like: {0}\".format(emb_matrix[item_to_id['woman']]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zueoWNr0rpbN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A common technique to inspect embeddings is **dimensionality reduction**, which reduces the many-dimensional vector (in our case 512) to a 2- or 3-dimensional vector which still captures the most important relationships. The simples dimensionality reduction technique is principal component analysis (PCA). How PCA exactly works, is not important here, but we will use it to map our embeddings to 2-dimensional space in the code below. We define a function that plots a subset of words in this 2-dimensional space."
      ]
    },
    {
      "metadata": {
        "id": "D0g6ZWggDmqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import the libraries that we need\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "    \n",
        "# perform principal component analysis\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(emb_matrix)\n",
        "\n",
        "def plot_pca(list_words):\n",
        "  '''\n",
        "  Plot all words in 'list_words' on the 2-dimensional PCA space.\n",
        "  '''\n",
        "  if len(list_words) > 10:\n",
        "    raise IOError(\"Maximum 10 words can be plotted.\")\n",
        "    \n",
        "  for w in list_words:\n",
        "    if w not in item_to_id:\n",
        "      list_words.remove(w)\n",
        "      print('Ignoring {0} because it is not in the vocabulary'.format(w))\n",
        "\n",
        "  colors = ['navy','turquoise','darkorange','red','black','blue','yellow','green','purple','pink']\n",
        "\n",
        "  for color, target_name in zip(colors[:len(list_words)], list_words):\n",
        "      plt.scatter(principalComponents[item_to_id[target_name], 0], \n",
        "                  principalComponents[item_to_id[target_name], 1], \n",
        "                  color=color, \n",
        "                  label=target_name)\n",
        "  plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gef82K2asdbc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can use the plot_pca function with a maximum of 10 words, for example:"
      ]
    },
    {
      "metadata": {
        "id": "CzdPrkgpoID4",
        "colab_type": "code",
        "outputId": "f5339a42-37f0-4b10-ec27-350191dd007f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "plot_pca(['man', 'woman', 'king', 'queen','men','women','child','children','boy','girl'])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBYRETHI/wep3E/lCfTvvAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5907cadd50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vFUhVLlDtEVN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You see in the example above that the plural nouns ('men', 'women', 'children') are concentrated in the upper left corner and that there is distinction between adult people and children ('man' and 'woman' more to the middle, 'boy', 'girl' and 'child' in the lower right corner)."
      ]
    },
    {
      "metadata": {
        "id": "JSXmvN17slez",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now try it yourself. Notice that only words that are in the vocabulary will be plotted."
      ]
    },
    {
      "metadata": {
        "id": "IuHhX4wwsogJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot_pca([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbNb5rbus5Da",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will notice that not all words display nice clear relationships in embedding space. Usually, for frequent words and relationships the results should be clear enough."
      ]
    },
    {
      "metadata": {
        "id": "Z3TUd6BPWQJc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another way to inspect word embeddings is to look at words that are closest to a specific target word. Closeness in a vector space can be calculated based on the cosine similarity. In the function below, we calculate for a given word the top 10 closest words."
      ]
    },
    {
      "metadata": {
        "id": "tKepamr6R-vX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_closest_words(word):\n",
        "  if word not in item_to_id:\n",
        "    raise IOError('This item is not in the vocabulary')\n",
        "    \n",
        "  else:\n",
        "    id_w = item_to_id[word]\n",
        "    emb_w = emb_matrix[id_w]\n",
        "    norm_emb_w = emb_w / np.linalg.norm(emb_w)\n",
        "    \n",
        "    top_10 = {}\n",
        "    \n",
        "    # iterate over all words\n",
        "    for idx in range(emb_matrix.shape[0]):\n",
        "      # ignore the word itself\n",
        "      if idx != id_w:\n",
        "        \n",
        "        norm_curr_w = emb_matrix[idx] / np.linalg.norm(emb_matrix[idx])\n",
        "        \n",
        "        # cosine similarity = dot product of normalized vectors\n",
        "        cos_sim = np.dot(norm_emb_w, norm_curr_w)\n",
        "        \n",
        "        # keep list of top 10 largest cos similarities\n",
        "        if len(top_10) >= 10:\n",
        "          for sim in top_10.iterkeys():\n",
        "            if cos_sim > sim:\n",
        "              \n",
        "              del top_10[sim]\n",
        "              top_10[cos_sim] = id_to_item[idx]\n",
        "              break\n",
        "        \n",
        "        else:\n",
        "          top_10[cos_sim] = id_to_item[idx]\n",
        "          \n",
        "        \n",
        "    print('Words with largest cosine similarity w.r.t. {0}'.format(word))\n",
        "    # sort the top 10 \n",
        "    for sim in sorted(top_10, key=float, reverse=True):\n",
        "      print('{0} ({1})'.format(top_10[sim], sim))\n",
        "    print()\n",
        "      \n",
        "     \n",
        "    \n",
        " \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YnvF6mD2XG3d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function above can be called as follows:"
      ]
    },
    {
      "metadata": {
        "id": "nqy54srADglI",
        "colab_type": "code",
        "outputId": "56564f06-a314-4cb9-e57a-08fc2883925e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "cell_type": "code",
      "source": [
        "find_closest_words('man')\n",
        "find_closest_words('help')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words with largest cosine similarity w.r.t. man\n",
            "artist (0.415580644469)\n",
            "driver (0.376201423505)\n",
            "investigator (0.344296556218)\n",
            "actor (0.33444143473)\n",
            "collins (0.315727783487)\n",
            "albert (0.311157892865)\n",
            "downey (0.308659725171)\n",
            "supervisor (0.295695289621)\n",
            "outsider (0.280173457463)\n",
            "barry (0.273468881175)\n",
            "\n",
            "Words with largest cosine similarity w.r.t. help\n",
            "helped (0.433193394718)\n",
            "helps (0.318657859787)\n",
            "laws (0.318054053432)\n",
            "sample (0.301497871059)\n",
            "staffs (0.296166505538)\n",
            "pesetas (0.293924983381)\n",
            "statutes (0.280734101887)\n",
            "drinks (0.273401642214)\n",
            "let (0.271040899948)\n",
            "encouraged (0.262834127029)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a2IsL_fxXJ0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will notice that the closest words to 'man' mostly correspond to specific types of men (professions, proper names) and that the two words closest to 'help' are conjugation of 'help'. Thus, we see that both semantic and syntactic relationships are present in the embedding space. Notice that we did not optimize our embedding space to contain such relationships! This is merely a by-product of training a neural language model. "
      ]
    },
    {
      "metadata": {
        "id": "Rw6ZRVy8Xk08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now try it for yourself. It is best to choose frequent words, because these will have better estimates. An error will be thrown if the word is not in the vocabulary. "
      ]
    },
    {
      "metadata": {
        "id": "dj5hLrA4XuTN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# find_closest_words('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PpoaAoNOX6qR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that not all words will have sensible nearest neighbours, because the model is trained on a relatively small dataset, not all words will have large enough frequency and the model is not optimized to encode these relationships."
      ]
    },
    {
      "metadata": {
        "id": "9sYewPcLl3WG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "Nr8m1gNedwnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before going into the details of training a neural language model, we will first show how you can use a trained model. The output of the neural network is given to a **softmax** function, which converts the output values (also called logits) to a values between 0 and 1. The sum of those values is 1, and thus the output of the softmax function can be treated as a probability distribution. "
      ]
    },
    {
      "metadata": {
        "id": "DeTdStZphMnH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then find the probability of a specific word  following the current input word by looking up its probability in the output vector of the softmax function. To find the most probable word, we look for the maximum probability. In practice, we usually work with **log probabilities**, because if we are computing the probability of a sequence of words, the multiplication of all probabilities easily becomes very small. Converting the probabilities to the log domain and summing them instead of multiplying alleviates this problem. The log probability of a sentence is then sum of the log probabilities of every word in the sentence, given their context."
      ]
    },
    {
      "metadata": {
        "id": "HQ0wAhRNmATx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's first train a network:"
      ]
    },
    {
      "metadata": {
        "id": "dQYIjUpZipvD",
        "colab_type": "code",
        "outputId": "379a5db5-2dec-4c4a-9f21-a56016d32b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "cell_type": "code",
      "source": [
        "# first make sure that we start training the model from scratch, by removing the models/ directory\n",
        "!rm -rf baseline\n",
        "\n",
        "# train the model\n",
        "run_lm(name='baseline', train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-29-e0b317f28b56>:73: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path baseline/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 1455.47111889\n",
            "Validation perplexity: 1027.12676012\n",
            "Epoch 2\n",
            "Train perplexity: 704.075517877\n",
            "Validation perplexity: 1290.00200014\n",
            "Epoch 3\n",
            "Train perplexity: 560.170174684\n",
            "Validation perplexity: 851.475167907\n",
            "Epoch 4\n",
            "Train perplexity: 465.316175759\n",
            "Validation perplexity: 847.824748726\n",
            "Epoch 5\n",
            "Train perplexity: 404.391975331\n",
            "Validation perplexity: 856.821292587\n",
            "Saved the model to  baseline/rnn.ckpt\n",
            "Test perplexity: 740.061534316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WhOFC3M2kMnC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "During training, not the log probabilities are printed but the **perplexity** of the model. Perplexity is commonly used to measure the quality of a language model, and corresponds to $$e^{\\frac{1}{N}~ln~P(x_1~\\ldots~x_N)}$$ You see that it is closely related to the log probability. The lower the perplexity, the better. Notice that in the example above the perplexities are quite high because we are training a small model on a small dataset."
      ]
    },
    {
      "metadata": {
        "id": "E1HZscovkEMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To test the model we define a function that print the log probability of a sentence. The sentence is converted to indices and then given to the model:"
      ]
    },
    {
      "metadata": {
        "id": "TaZ-Wle_C4Pk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_log_prob(test_sent):\n",
        "  \n",
        "  # convert words to indices\n",
        "  test_idx = []\n",
        "  for w in test_sent.split(' '):\n",
        "    if w not in item_to_id:\n",
        "      raise IOError(\"{0} is not part of the vocabulary\".format(w))\n",
        "    else:\n",
        "      test_idx.append(item_to_id[w])\n",
        "\n",
        "  # feed sentence to the model\n",
        "  run_lm(name='baseline',\n",
        "                test_ids=test_idx,\n",
        "                test_log_prob=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sp8fOR8mPZ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To get the log probability of a specific sentence, use the following commands:"
      ]
    },
    {
      "metadata": {
        "id": "XMoIfVfSZm1D",
        "colab_type": "code",
        "outputId": "ec4a8c02-c3cd-4b75-8387-02a8a1ccf5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "get_log_prob(test_sent='this is a test <eos>')\n",
        "get_log_prob(test_sent='test a a a <eos>')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from baseline/rnn.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:Saving checkpoint to path baseline/model.ckpt\n",
            "Log probability: -19.7628390789\n",
            "INFO:tensorflow:Restoring parameters from baseline/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:Saving checkpoint to path baseline/model.ckpt\n",
            "Log probability: -24.8727293015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvXDxzgFmcQv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should see that the log probability of 'test a a a' is lower than 'this is a test', which makes sense. You can test your own sentences here:"
      ]
    },
    {
      "metadata": {
        "id": "pdmBlzRwmi94",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get_log_prob('your own test sentence')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbEp9tdFKXA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training networks"
      ]
    },
    {
      "metadata": {
        "id": "a61RdtDCKhmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training neural networks requires a lot of hyperparameter tuning. The hyperparameters of a neural network are for example the type of cell, its size, the method that is used for updating its parameters (also called 'optimizer' ), the type and strength of regularization, ... . All these hyperparameters have to be chosen before the network can built, trained and tested, and they all have to some extent an influence on the  performance of the model."
      ]
    },
    {
      "metadata": {
        "id": "BVTDtTYYw77W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The default arguments for our network are the following:\n",
        "\n",
        "\n",
        "* cell: 'LSTM'\n",
        "* optimizer: 'Adam'\n",
        "* lr: 0.01\n",
        "* embedding_size: 64\n",
        "* hidden_size: 128\n",
        "* dropout_rate: 0.5\n",
        "\n",
        "We will explain each of these arguments in the following sections.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6ZaHDz1Bsqnt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Type of cell"
      ]
    },
    {
      "metadata": {
        "id": "j3bN2Mvmo_8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks are neural networks that take as input a combination of the standard input and the hidden state of the previous time step. The simples form of recurrent neural network, often called **vanilla recurrent neural network**, looks like this (picture taken from the Chris Olah's [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)):"
      ]
    },
    {
      "metadata": {
        "id": "swms9ZXrq1X0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)"
      ]
    },
    {
      "metadata": {
        "id": "DIPmg4vwq3l5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The green blocks in the picture represent the neural network. You see that the input to the neural network is the current input word $\\mathbf{x}_t$ (this is the word embedding as discussed before) and the state of the previous time step $\\mathbf{h}_{t-1}$. The network multiplies both inputs with weights and applies a non-linearity, in this case $\\tanh$. "
      ]
    },
    {
      "metadata": {
        "id": "nd57sRKcsEKY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The network that we trained before is not a vanilla recurrent neural network, but a **long short-term memory (LSTM)** network, which is a more powerful variant. If you're interested in knowing how the LSTM works, the blog post mentioned above is a great introduction."
      ]
    },
    {
      "metadata": {
        "id": "42u843fb9qq2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For comparison, let's now train a simple RNN instead of an LSTM:"
      ]
    },
    {
      "metadata": {
        "id": "KRUa0rDYc7SC",
        "colab_type": "code",
        "outputId": "0ffdddc0-09f5-4be4-81fb-0c0c670fa9b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='RNN', cell='RNN', train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-cc3eaaaef924>:71: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path RNN/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 2364.69203851\n",
            "Validation perplexity: 1222.99611609\n",
            "Epoch 2\n",
            "Train perplexity: 849.249732307\n",
            "Validation perplexity: 1336.61490683\n",
            "Epoch 3\n",
            "Train perplexity: 744.066620528\n",
            "Validation perplexity: 1389.19858154\n",
            "Epoch 4\n",
            "Train perplexity: 657.9398041\n",
            "Validation perplexity: 1337.80487991\n",
            "Epoch 5\n",
            "Train perplexity: 609.659394358\n",
            "Validation perplexity: 1470.17517718\n",
            "Saved the model to  RNN/rnn.ckpt\n",
            "Test perplexity: 1185.73019903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wr4JKWqNs7c_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that the simple RNN performs much worse than the LSTM (the perplexities are much higher)."
      ]
    },
    {
      "metadata": {
        "id": "zI30kaseLmVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "osTYguHZprBE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another important hyperparameter for neural networks is the type of optimizer. Training a neural network implies that you give an input to the network, calculate the output and the difference with the expected output, which is equal to the **error** or **loss**. To update the parameters based on the error, the **gradient** of the loss with respect to the parameters is calculated. The gradient tells you in which direction you need to move to maximize the loss. In our case, we want to minimize the loss so we will move in the opposite direction (negative gradient). The optimizer then decides how this gradient is used to change the parameters. "
      ]
    },
    {
      "metadata": {
        "id": "gpv1uOXhufG1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The simplest option is to subtract (a scaled version of) the gradient from the parameters. This optimizer is called **stochastic gradient descent**. In the experiments above, we used another, more complicated, optimizer called **Adam**."
      ]
    },
    {
      "metadata": {
        "id": "FWJaqfh7vz8R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now train a network with stochastic gradient descent instead of Adam:"
      ]
    },
    {
      "metadata": {
        "id": "dWILisHi1xHp",
        "colab_type": "code",
        "outputId": "812a8453-5c60-40c5-b48b-e0a339bdee42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='SGD', optimizer='SGD', train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:Saving checkpoint to path SGD/model.ckpt\n",
            "Epoch 1\n",
            "Train perplexity: 9180.55422437\n",
            "Validation perplexity: 8166.46473993\n",
            "Epoch 2\n",
            "Train perplexity: 6228.1793825\n",
            "Validation perplexity: 4225.31975983\n",
            "Epoch 3\n",
            "Train perplexity: 2887.18339618\n",
            "Validation perplexity: 2088.03051321\n",
            "Epoch 4\n",
            "Train perplexity: 1750.78211763\n",
            "Validation perplexity: 1547.72971098\n",
            "Epoch 5\n",
            "Train perplexity: 1383.64458163\n",
            "Validation perplexity: 1296.45455176\n",
            "Saved the model to  SGD/rnn.ckpt\n",
            "Test perplexity: 1317.23809367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nw4-SPVMv-PU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You see that the perplexities are worse than for our baseline model. "
      ]
    },
    {
      "metadata": {
        "id": "8wKt7L9aPLfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learning rate"
      ]
    },
    {
      "metadata": {
        "id": "EXDniSW54NfB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Judging from the perplexities above, it seems like the Adam optimizer is the best choice for training our network. However, the interplay between the different hyperparameters of a neural network is complicated, and it is very well possible that a specific optimizer needs a different learning rate. \n",
        "Let's try SGD with a learning rate of 1 instead of the default 0.01:"
      ]
    },
    {
      "metadata": {
        "id": "pPl3KiSs5kQk",
        "colab_type": "code",
        "outputId": "33b0b6a3-77d6-4fe5-b70b-b236e9daacc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='SGD_1', optimizer='SGD', lr=1.0, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "INFO:tensorflow:Saving checkpoint to path SGD_1/model.ckpt\n",
            "Train perplexity: 2567.86778209\n",
            "Validation perplexity: 1298.50340242\n",
            "Epoch 2\n",
            "Train perplexity: 1135.15623648\n",
            "Validation perplexity: 1000.63395997\n",
            "Epoch 3\n",
            "Train perplexity: 924.120475531\n",
            "Validation perplexity: 833.298030109\n",
            "Epoch 4\n",
            "Train perplexity: 776.529975947\n",
            "Validation perplexity: 734.218210846\n",
            "Epoch 5\n",
            "Train perplexity: 653.31973554\n",
            "Validation perplexity: 750.161268444\n",
            "Saved the model to  SGD_1/rnn.ckpt\n",
            "Test perplexity: 712.883834901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcNjIbzgDUuk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that a larger learning rate helps for SGD, and gives even better results than Adam. Maybe using a larger learning rate helps in general? Let's try the same learning rate in combination with Adam:"
      ]
    },
    {
      "metadata": {
        "id": "lSH6WKIdxlAE",
        "colab_type": "code",
        "outputId": "4c8a4e40-0745-4d84-da58-7b7ec2809edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='Adam_1', lr=1.0, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path Adam_1/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 1.84682483802e+32\n",
            "Validation perplexity: 3.7944516351e+38\n",
            "Epoch 2\n",
            "Train perplexity: 9.74797980989e+32\n",
            "Validation perplexity: 1.51390132532e+33\n",
            "Epoch 3\n",
            "Train perplexity: 4.67215085192e+34\n",
            "Validation perplexity: 4.03068627174e+36\n",
            "Epoch 4\n",
            "Train perplexity: 1.96186139553e+37\n",
            "Validation perplexity: 8.81369390325e+37\n",
            "Epoch 5\n",
            "Train perplexity: 1.05058210657e+37\n",
            "Validation perplexity: 1.57416001856e+36\n",
            "Saved the model to  Adam_1/rnn.ckpt\n",
            "Test perplexity: 1.38115466103e+36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-XqENWWbyCdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This gives terrible results, which clearly indicates that it is important to tune the learning rate and the optimizer jointly. For Adam, a smaller learning rate is better. What if we reduce the learning rate further, from 0.01 to 0.001?"
      ]
    },
    {
      "metadata": {
        "id": "UyUTwGvF7Iyy",
        "colab_type": "code",
        "outputId": "99f04499-e6a5-4c2e-f478-b544f3bf683d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='Adam_0.001', lr=0.001, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path Adam_0.001/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 3569.66542718\n",
            "Validation perplexity: 978.966751897\n",
            "Epoch 2\n",
            "Train perplexity: 697.015506413\n",
            "Validation perplexity: 947.301051956\n",
            "Epoch 3\n",
            "Train perplexity: 661.234727646\n",
            "Validation perplexity: 956.275353676\n",
            "Epoch 4\n",
            "Train perplexity: 645.161367668\n",
            "Validation perplexity: 956.699421319\n",
            "Epoch 5\n",
            "Train perplexity: 673.492560025\n",
            "Validation perplexity: 953.859007018\n",
            "('Saved the model to ', 'Adam_0.001/rnn.ckpt')\n",
            "Test perplexity: 837.943287268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nNkyWjQNCjja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is slightly worse than the baseline perplexity. "
      ]
    },
    {
      "metadata": {
        "id": "MGAY_hhfNLj5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Size of the embedding"
      ]
    },
    {
      "metadata": {
        "id": "jZ87PmaPJlib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at the influence of the size of the LSTM on its performance. By default, we train a model with embeddings of size 64 and a hidden layer of size 128. Let's see what happens if we reduce the size of the embedding:"
      ]
    },
    {
      "metadata": {
        "id": "q2d_Kx8zJGk-",
        "colab_type": "code",
        "outputId": "431ad12a-8644-4c83-b3bc-8c28ecc629df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='emb16', embedding_size=16, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path emb16/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 1452.31607755\n",
            "Validation perplexity: 980.647685402\n",
            "Epoch 2\n",
            "Train perplexity: 706.728881564\n",
            "Validation perplexity: 1290.7853409\n",
            "Epoch 3\n",
            "Train perplexity: 606.009862433\n",
            "Validation perplexity: 1130.55933214\n",
            "Epoch 4\n",
            "Train perplexity: 552.040121515\n",
            "Validation perplexity: 870.350705537\n",
            "Epoch 5\n",
            "Train perplexity: 482.286987845\n",
            "Validation perplexity: 902.309769022\n",
            "('Saved the model to ', 'emb16/rnn.ckpt')\n",
            "Test perplexity: 777.968927924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eHf3oNWgzEyG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A smaller embedding size gives slightly worse results.  What about a larger embedding size?"
      ]
    },
    {
      "metadata": {
        "id": "3D6D-G01zIN5",
        "colab_type": "code",
        "outputId": "82806f2d-473a-49cc-e98f-360aacb8f66e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='emb128', embedding_size=128, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path emb128/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 1504.28973842\n",
            "Validation perplexity: 1031.59011262\n",
            "Epoch 2\n",
            "Train perplexity: 692.559943784\n",
            "Validation perplexity: 867.1853053\n",
            "Epoch 3\n",
            "Train perplexity: 537.566607546\n",
            "Validation perplexity: 848.980220534\n",
            "Epoch 4\n",
            "Train perplexity: 442.985098957\n",
            "Validation perplexity: 832.782362787\n",
            "Epoch 5\n",
            "Train perplexity: 379.935115053\n",
            "Validation perplexity: 878.801423174\n",
            "('Saved the model to ', 'emb128/rnn.ckpt')\n",
            "Test perplexity: 736.466715072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WSpgrmJa7-8L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should give better results, but notice that the differences are small."
      ]
    },
    {
      "metadata": {
        "id": "8ranM0ERNIta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Size of the hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "n9huic9_8Het",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also change the size of the hidden layer/the number of neurons in the network. In principle you also choose to increase the number of layers, but this is mainly useful for large datasets."
      ]
    },
    {
      "metadata": {
        "id": "PF81eBTK8b5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's first test a smaller hidden layer:"
      ]
    },
    {
      "metadata": {
        "id": "vbkbvBaWLVaj",
        "colab_type": "code",
        "outputId": "d3d271e8-f0ca-4491-c613-95ffa373fcb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='hidden64', hidden_size=64, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path hidden64/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 1677.70257795\n",
            "Validation perplexity: 1099.17567984\n",
            "Epoch 2\n",
            "Train perplexity: 740.42272477\n",
            "Validation perplexity: 928.991178583\n",
            "Epoch 3\n",
            "Train perplexity: 613.996031245\n",
            "Validation perplexity: 909.624262789\n",
            "Epoch 4\n",
            "Train perplexity: 514.14685331\n",
            "Validation perplexity: 870.480120328\n",
            "Epoch 5\n",
            "Train perplexity: 445.795121091\n",
            "Validation perplexity: 881.397887168\n",
            "('Saved the model to ', 'hidden64/rnn.ckpt')\n",
            "Test perplexity: 749.518940256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YjSUxuNm8jVA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We observe almost no difference for a smaller hidden layer. "
      ]
    },
    {
      "metadata": {
        "id": "1XrNmCN88ewu",
        "colab_type": "code",
        "outputId": "e6ef1c9c-256c-40ba-d891-0cf66fd7bc78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='hidden256', hidden_size=256, train_ids=train_ids, valid_ids=valid_ids, test_ids=test_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "INFO:tensorflow:Saving checkpoint to path hidden256/model.ckpt\n",
            "Train perplexity: 1365.41560209\n",
            "Validation perplexity: 1013.17449997\n",
            "Epoch 2\n",
            "Train perplexity: 631.971166415\n",
            "Validation perplexity: 2357.3619663\n",
            "Epoch 3\n",
            "Train perplexity: 532.601764739\n",
            "Validation perplexity: 902.201833147\n",
            "Epoch 4\n",
            "Train perplexity: 459.805993293\n",
            "Validation perplexity: 862.397190502\n",
            "Epoch 5\n",
            "Train perplexity: 395.757067641\n",
            "Validation perplexity: 1043.19493401\n",
            "('Saved the model to ', 'hidden256/rnn.ckpt')\n",
            "Test perplexity: 736.215451823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CxoqYlUv9FUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A larger hidden layer moderately improves the results, but again the difference is small. Notice that parameters such as the size of the embedding and the size of the hidden layer will have a larger effect for large data sets. To see this, we will now train on the full Penn TreeBank data set by using {train/valid/test}_ids_large:"
      ]
    },
    {
      "metadata": {
        "id": "RrmToOiiLlj0",
        "colab_type": "code",
        "outputId": "de7ba5d5-4016-4149-dc69-d941242af69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='large', train_ids=train_ids_large, valid_ids=valid_ids_large, test_ids=test_ids_large)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path large/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 448.133119457\n",
            "Validation perplexity: 309.512903591\n",
            "Epoch 2\n",
            "Train perplexity: 264.139868696\n",
            "Validation perplexity: 260.352211332\n",
            "Epoch 3\n",
            "Train perplexity: 221.642533035\n",
            "Validation perplexity: 242.831497651\n",
            "Epoch 4\n",
            "Train perplexity: 202.418798594\n",
            "Validation perplexity: 230.736476619\n",
            "Epoch 5\n",
            "Train perplexity: 190.955553896\n",
            "Validation perplexity: 224.564248629\n",
            "('Saved the model to ', 'large/rnn.ckpt')\n",
            "Test perplexity: 209.699917701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kZdunTOcMEAL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now train a model with a larger hidden size:"
      ]
    },
    {
      "metadata": {
        "id": "UqZOMze8AMIJ",
        "colab_type": "code",
        "outputId": "a476198d-b475-45d1-b087-265e924fb823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(name='large_hidden256', hidden_size=256, \n",
        "              train_ids=train_ids_large, valid_ids=valid_ids_large, test_ids=test_ids_large)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1INFO:tensorflow:Saving checkpoint to path large_hidden256/model.ckpt\n",
            "\n",
            "Train perplexity: 397.047016231\n",
            "Validation perplexity: 273.377945013\n",
            "Epoch 2\n",
            "Train perplexity: 247.25939606\n",
            "Validation perplexity: 228.659513446\n",
            "Epoch 3\n",
            "Train perplexity: 190.373732934\n",
            "Validation perplexity: 210.419504074\n",
            "Epoch 4\n",
            "Train perplexity: 171.263610246\n",
            "Validation perplexity: 203.691183703\n",
            "Epoch 5\n",
            "Train perplexity: 159.386447743\n",
            "Validation perplexity: 199.9164812\n",
            "('Saved the model to ', 'large_hidden256/rnn.ckpt')\n",
            "Test perplexity: 188.050517488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1uyXYwyiNgfO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that for the larger data set, increasing the model size gives larger gains."
      ]
    },
    {
      "metadata": {
        "id": "v1Ya3bs084Rb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Try it yourself"
      ]
    },
    {
      "metadata": {
        "id": "KnlnlYWSMOA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now try some different values for the hyperparameters that we discussed:"
      ]
    },
    {
      "metadata": {
        "id": "K_uSCdPgMNXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run_lm(name='', train_ids=train_ids_large, valid_ids=valid_ids_large, test_ids=test_ids_large)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJ4EmqovMa7x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The hyperparameters that we discussed here are only a small subset, there exist many more that can have an influence on the performance of the neural network. Another important class of hyperparameters is related to **regularization**. Since neural networks contain many parameters, they can easily start **overfitting**, which means that the network starts memorizing the training set and cannot generalize well to new data sets anymore. Two important methods are dropout (setting a proportion of the neurons to 0 during training) and early stopping (stop training if the performance on the validation set decreases), but there exist many more."
      ]
    }
  ]
}