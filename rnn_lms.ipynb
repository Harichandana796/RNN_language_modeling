{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_lms.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/lverwimp/RNN_language_modeling/blob/master/rnn_lms.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "esuUK2ev_Nyt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ]
    },
    {
      "metadata": {
        "id": "_yKJ-Plo_HTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib, collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BddwEvSP_ekG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Enable eager execution in TensorFlow:"
      ]
    },
    {
      "metadata": {
        "id": "CQPkGtgt_huR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UScGV-tTaZba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some global variables:"
      ]
    },
    {
      "metadata": {
        "id": "KNbwMREbacvY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 20\n",
        "NUM_STEPS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-oKtBwJQAJED",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get training, validation and test data:"
      ]
    },
    {
      "metadata": {
        "id": "t6AIeiR2BaMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_url = 'http://homes.esat.kuleuven.be/~lverwimp/course_speech_recognition/train.txt'\n",
        "valid_url = 'http://homes.esat.kuleuven.be/~lverwimp/course_speech_recognition/valid.txt'\n",
        "test_url = 'http://homes.esat.kuleuven.be/~lverwimp/course_speech_recognition/test.txt'\n",
        "train_file = urllib.urlopen(train_url).read()\n",
        "valid_file = urllib.urlopen(valid_url).read()\n",
        "test_file = urllib.urlopen(test_url).read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Ye3oJavH-8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "GXEcoaWKIAtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "c3f51e1c-a3b2-4464-eaa8-ef50efc5a99f"
      },
      "cell_type": "code",
      "source": [
        "print('{0}...'.format(valid_file[:500]))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " consumers may want to move their telephones a little closer to the tv set \n",
            " <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            " two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues \n",
            " and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day 's show \n",
            " interactive telephone technology...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ZF1PoUJDRkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<unk\\> is a symbol for the unknown words class, 'N' is a symbol used for the numbers class.\n",
        "  \n",
        "Convert data to correct format:"
      ]
    },
    {
      "metadata": {
        "id": "Y531EV5aDVd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert the string to a list and replace newlines with the end-of-sentence symbol\n",
        "train_text = [w for w in train_file.replace('\\n',' <eos>').split(' ')]\n",
        "valid_text = [w for w in valid_file.replace('\\n',' <eos>').split(' ')]\n",
        "test_text = [w for w in test_file.replace('\\n',' <eos>').split(' ')]\n",
        "\n",
        "# count the frequencies of the words in the training data\n",
        "counter = collections.Counter(train_text)\n",
        "\n",
        "# sort according to decreasing frequency\n",
        "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "# words = list of all the words (in decreasing frequency)\n",
        "items, _ = list(zip(*count_pairs))\n",
        "\n",
        "# make a dictionary with a mapping from each word to an id; word with highest frequency gets lowest id etc.\n",
        "item_to_id = dict(zip(items, range(len(items))))\n",
        "\n",
        "# convert the words to indices\n",
        "train_ids = [item_to_id[item] for item in train_text]\n",
        "valid_ids = [item_to_id[item] for item in valid_text]\n",
        "test_ids = [item_to_id[item] for item in test_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6KQShlZFcGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the data is converted to ids, it looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "Vlo5SpAHFhLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "851271c1-7741-429e-c315-56000c4c1b51"
      },
      "cell_type": "code",
      "source": [
        "print(valid_ids[:100])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1133, 94, 359, 6, 330, 52, 9837, 7, 327, 2477, 6, 0, 663, 389, 2, 3, 1, 1, 2975, 2159, 10, 382, 1069, 2348, 90, 100, 848, 199, 1, 12, 0, 3384, 1120, 8, 4, 73, 21, 212, 347, 37, 259, 1, 1, 2, 3, 76, 423, 196, 3918, 5, 250, 1796, 1, 581, 3529, 893, 2375, 7, 4, 298, 12, 2710, 17, 1187, 1, 251, 2, 3, 9, 0, 36, 9923, 3748, 465, 711, 2999, 2038, 3918, 135, 6146, 12, 495, 5895, 17, 0, 131, 273, 10, 465, 2, 3, 9959, 733, 504, 31, 642, 7, 36, 6499]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jBnJVfPuNzmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Class for the language model:"
      ]
    },
    {
      "metadata": {
        "id": "e_IlLSYXN1h7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class rnn_lm(object):\n",
        "  '''\n",
        "  This is a class to build and execute a recurrent neural network language model.\n",
        "  '''\n",
        "  \n",
        "  def __init__(self,\n",
        "              cell='LSTM',\n",
        "              vocab_size=10000,\n",
        "              embedding_size=64,\n",
        "              hidden_size=128,\n",
        "              dropout_rate=0.5):\n",
        "    self.which_cell = cell\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.batch_size = BATCH_SIZE\n",
        "    self.max_grad_norm = 5\n",
        "    self.lr = 1\n",
        "    \n",
        "  def build_training_graph(self):\n",
        "    '''\n",
        "    This function builds the training graph.\n",
        "    '''\n",
        "    \n",
        "    # input embedding weights\n",
        "    self.embedding = tf.get_variable(\"embedding\", \n",
        "                                     [self.vocab_size, self.embedding_size], \n",
        "                                     dtype=tf.float32)\n",
        "    \n",
        "    # hidden layer\n",
        "    if self.which_cell == 'LSTM':\n",
        "      self.cell = tf.contrib.rnn.BasicLSTMCell(self.hidden_size)\n",
        "    elif self.which_cell == 'RNN':\n",
        "      self.cell = tf.contrib.rnn.BasicRNNCell(self.hidden_size)\n",
        "    elif self.which_cell == 'GRU':\n",
        "      self.cell = tf.contrib.rnn.GRUCell(self.hidden_size)\n",
        "    else:\n",
        "      raise IOError(\"Specify which type of RNN you want to use: RNN, GRU or LSTM.\")\n",
        "      \n",
        "    # apply dropout  \n",
        "    self.cell = tf.contrib.rnn.DropoutWrapper(self.cell, \n",
        "                                              output_keep_prob=self.dropout_rate)\n",
        "    \n",
        "    # initial state contains all zeros\n",
        "    self.initial_state = self.cell.zero_state(self.batch_size, tf.float32)\n",
        "    \n",
        "    # output weight matrix and bias\n",
        "    self.softmax_w = tf.get_variable(\"softmax_w\",\n",
        "                                     [self.hidden_size, self.vocab_size], \n",
        "                                     dtype=tf.float32)\n",
        "    self.softmax_b = tf.get_variable(\"softmax_b\",\n",
        "                                     [self.vocab_size], \n",
        "                                     dtype=tf.float32)\n",
        "    \n",
        "    return self.initial_state\n",
        "    \n",
        "  def train_model(self, inputs, targets, state):\n",
        "    \n",
        "    # Step 1: feed the inputs to model and calculate output logits\n",
        "    output, state = self.feed_to_network(inputs, state)\n",
        "    \n",
        "    # Step 2: calculate loss\n",
        "    self.calc_loss(output, targets)\n",
        "    \n",
        "    # Step 3: calculate gradients and update the parameters of the network\n",
        "    #self.update_params(loss)\n",
        "    \n",
        "    return state\n",
        "    \n",
        "    \n",
        "  def feed_to_network(self, inputs, state):\n",
        "    \n",
        "    # map input indices to continuous input vectors\n",
        "    inputs = tf.nn.embedding_lookup(self.embedding, inputs)\n",
        "\n",
        "\t  # use dropout on the input embeddings\n",
        "    inputs = tf.nn.dropout(inputs, self.dropout_rate)\n",
        "    \n",
        "    # feed inputs to network: outputs = predictions, state = new hidden state\n",
        "    outputs, state = tf.nn.dynamic_rnn(self.cell, inputs, sequence_length=None, initial_state=state)\n",
        "    \n",
        "    output = tf.reshape(tf.concat(outputs, 1), [-1, self.hidden_size])\n",
        "    \n",
        "    # calculate logits\n",
        "    #logits = tf.matmul(output, self.softmax_w) + self.softmax_b\n",
        "    \n",
        "    return output, state\n",
        "  \n",
        "  def calc_loss(self, output, targets):\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = tf.matmul(output, self.softmax_w) + self.softmax_b\n",
        "      \n",
        "      # calculate cross entropy loss\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=tf.reshape(targets, [-1]), logits=logits)\n",
        "      \n",
        "      # average loss per batch\n",
        "      avg_loss = tf.reduce_sum(loss) / self.batch_size\n",
        "    \n",
        "    grads = tape.gradient(avg_loss, tf.trainable_variables())\n",
        "    print('grads: {0}'.format(grads))\n",
        "    print(tf.trainable_variables())\n",
        "    \n",
        "    grads = tf.clip_by_global_norm(grads, self.max_grad_norm)\n",
        "    print('grads: {0}'.format(grads))\n",
        "    \n",
        "    # optimize with stochastic gradient descent\n",
        "    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "    \n",
        "    optimizer.apply_gradients(\n",
        "\t\t\t\tzip(grads, tf.trainable_variables()))\n",
        "    \n",
        "    #return avg_loss\n",
        "  \n",
        "  def update_params(self, cost):\n",
        "    \n",
        "    # calculte gradients for all trainable variables \n",
        "    # + clip them if their global norm > 5 (prevents exploding gradients)\n",
        "    grads, _ = tf.clip_by_global_norm(\n",
        "        tf.gradients(cost, tf.trainable_variables()), self.max_grad_norm)\n",
        "    \n",
        "    # optimize with stochastic gradient descent\n",
        "    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "    \n",
        "    optimizer.apply_gradients(\n",
        "\t\t\t\tzip(grads, tf.trainable_variables()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tIHeGqTCW8J-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create network:"
      ]
    },
    {
      "metadata": {
        "id": "sEvlMOEJa4qU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_batches(data, vocab_size, rnn, state):\n",
        "  \n",
        "  data_array = np.array(data)\n",
        "  \n",
        "  len_batch_instance = vocab_size / BATCH_SIZE\n",
        "  \n",
        "  data_array = data_array[:BATCH_SIZE*len_batch_instance]\n",
        "  \n",
        "  # divide data in BATCH_SIZE parts\n",
        "  data_reshaped = np.reshape(data_array, (BATCH_SIZE, len_batch_instance))\n",
        "  \n",
        "  # number of mini-batches that can be generated\n",
        "  num_batches_in_data = len_batch_instance / NUM_STEPS\n",
        "\n",
        "  # generate mini-batches\n",
        "  for i in range(num_batches_in_data):\n",
        "    input_batch = data_reshaped[:,i*BATCH_SIZE:i*BATCH_SIZE+BATCH_SIZE]\n",
        "    # target = input shifted 1 time step\n",
        "    target_batch = data_reshaped[:,i*BATCH_SIZE+1:i*BATCH_SIZE+BATCH_SIZE+1]\n",
        "    \n",
        "    state = rnn.train_model(input_batch, target_batch, state)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5K8dJkswaT7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "f0507954-cc23-4ed0-f5d4-f121cf57e609"
      },
      "cell_type": "code",
      "source": [
        "rnn = rnn_lm()\n",
        "\n",
        "init_state = rnn.build_training_graph()\n",
        "\n",
        "create_batches(train_ids, len(item_to_id), rnn, init_state)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "grads: []\n",
            "[]\n",
            "grads: ([], <tf.Tensor: id=10808, shape=(), dtype=float32, numpy=0.0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-3a7beb2d451c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minit_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_training_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_to_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-111-4e1f75a4d954>\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(data, vocab_size, rnn, state)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_reshaped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-df08c9915330>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, inputs, targets, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Step 2: calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Step 3: calculate gradients and update the parameters of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-133-df08c9915330>\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(self, output, targets)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     optimizer.apply_gradients(\n\u001b[0;32m--> 113\u001b[0;31m \t\t\t\tzip(grads, tf.trainable_variables()))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m#return avg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make sure repeat iteration works.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     \u001b[0mconverted_grads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No variables provided."
          ]
        }
      ]
    }
  ]
}