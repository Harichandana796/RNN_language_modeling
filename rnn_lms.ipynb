{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn_lms.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lverwimp/RNN_language_modeling/blob/master/rnn_lms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "FyAmshu3bFz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Language Modeling with Recurrent Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "0fc7h-VVbPBV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will see how you can train a recurrent neural network language model.\n",
        "\n",
        "We will start by importing TensorFlow, which is Google's open-source library for machine learning. Next, we will explain how to do data processing for language modeling and show you how we can train and test models."
      ]
    },
    {
      "metadata": {
        "id": "vzV_zLwTbr94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing TensorFlow and other requirements"
      ]
    },
    {
      "metadata": {
        "id": "esuUK2ev_Nyt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We start by importing TensorFlow and checking if we are running on GPU:"
      ]
    },
    {
      "metadata": {
        "id": "_yKJ-Plo_HTS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FKKFmkwat9W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the code above raised an error, you should make sure that you are using a GPU in the following way: select 'Runtime' in the top bar, then 'Change runtime type' and choose 'GPU' as hardware accelerator. Training neural networks is much faster on a GPU (graphics processing unit) than on a CPU."
      ]
    },
    {
      "metadata": {
        "id": "OZ6KKnfoahTc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we do the other imports we need. The following code will allow you to upload files: you have to upload batchGenerator.py, rnn_lm.py and run_lm.py."
      ]
    },
    {
      "metadata": {
        "id": "BnEd3YIyDsDd",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "7f90e492-1139-4f43-b210-f9d8f8ca1f86"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib, collections, os\n",
        "\n",
        "# upload batchGenerator.py, rnn_lm.py, run_lm.py (all at once)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6c5fdfa2-5004-4a61-991d-0c077f04b1a8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6c5fdfa2-5004-4a61-991d-0c077f04b1a8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving batchGenerator.py to batchGenerator.py\n",
            "Saving rnn_lm.py to rnn_lm.py\n",
            "Saving run_lm.py to run_lm.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4PBIC9HGnHuY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the files are uploaded correctly, the following imports should succeed:"
      ]
    },
    {
      "metadata": {
        "id": "6NAjYXSxmBxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import rnn_lm, batchGenerator, run_lm\n",
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqMR__zjnNAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the imports did not succeed, you should restart the runtime ('Runtime' in the top bar and then 'Restart runtime') and/or delete the files in the overview to the left (tab 'Files')."
      ]
    },
    {
      "metadata": {
        "id": "o3-bMavvngQx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you imported all libraries, you can now start the following section on data processing."
      ]
    },
    {
      "metadata": {
        "id": "7hmCKPx4KEcG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data processing"
      ]
    },
    {
      "metadata": {
        "id": "-oKtBwJQAJED",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will train our language models on **Penn TreeBank**, which is a publicly available benchmark dataset. A benchmark dataset can be used to easily compare models, since everyone has access to the same data. Many published papers use Penn TreeBank as dataset.\n",
        "\n",
        "It consists of among others newspaper articles, transcribed telephone conversations and manuals. The training set contains ca. 900.000 words, the validation set ca. 70.000 words and the test set ca. 80k words. This is a very small dataset (nowadays language models can be trained on billions of words), but it is large enough for our purposes.\n",
        "\n",
        "We now download the training, validation and test data:"
      ]
    },
    {
      "metadata": {
        "id": "t6AIeiR2BaMm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_url = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/data/train.txt'\n",
        "valid_url = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/data/valid.txt'\n",
        "test_url = 'http://homes.esat.kuleuven.be/~spchlab/H02A6/lab/session6/data/test.txt'\n",
        "train_file = urllib.urlopen(train_url).read()\n",
        "valid_file = urllib.urlopen(valid_url).read()\n",
        "test_file = urllib.urlopen(test_url).read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Ye3oJavH-8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "GXEcoaWKIAtE",
        "colab_type": "code",
        "outputId": "a0650c5e-1d27-48d2-9c09-c1bac2b17636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "print('{0}...'.format(valid_file[:500]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " consumers may want to move their telephones a little closer to the tv set \n",
            " <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            " two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues \n",
            " and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day 's show \n",
            " interactive telephone technology...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ZF1PoUJDRkH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data has been **normalized**: all words not in the vocabulary are mapped to an unknown words class (<unk\\>), all numbers are mapped to the 'N' class, each line contains a single sentence, punctuation has been removed, and so on. \n",
        "\n",
        "The purpose of normalization is among others to get rid of all information that is not necessary (such as punctuation), to solve redundancies (for example the same word can occur with different spellings, e.g. 'normalisation' or 'normalization', and we want to get rid of such variants) and to make sure the language model will be able to generalize better. An example of the latter case is the mapping of all numbers to 'N':  in the example above, 'in N years', 'N' can correspond to any number. Assume that in our training data, we see 'in 20 years' and 'in 11 years', and in our test data, we see 'in 5 years'. If '20', '11' and '5' are not mapped to 'N', we have never seen 'in 5 years' before, and the probability estimate will be worse.\n",
        "  \n",
        "We will now read the data, add end-of-sentence symbols (since we want to be able to predict the end of a sentence too), and count the frequency of every word in the training data:"
      ]
    },
    {
      "metadata": {
        "id": "Y531EV5aDVd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# convert the string to a list and replace newlines with the end-of-sentence symbol <eos>\n",
        "# ignore empty elements ''\n",
        "train_text = [w for w in train_file.replace('\\n',' <eos>').split(' ') if w != '']\n",
        "valid_text = [w for w in valid_file.replace('\\n',' <eos>').split(' ') if w != '']\n",
        "test_text = [w for w in test_file.replace('\\n',' <eos>').split(' ') if w != '']\n",
        "\n",
        "# count the frequencies of the words in the training data\n",
        "counter = collections.Counter(train_text)\n",
        "\n",
        "# sort according to decreasing frequency\n",
        "count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ci3iri5qAu2n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can take a look at the frequencies of the words in the training set, and compare them with the frequencies of the words in the validation set. The top 20 words is quite similar:"
      ]
    },
    {
      "metadata": {
        "id": "8AC1Y_TxAHYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f24df5b6-23b2-4eca-ac35-7f79251eed02"
      },
      "cell_type": "code",
      "source": [
        "# count the frequencies of the words in the validation data\n",
        "counter_valid = collections.Counter(valid_text)\n",
        "\n",
        "# sort according to decreasing frequency\n",
        "count_pairs_valid = sorted(counter_valid.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "print('Top 20 most frequent words:')\n",
        "print('Train (freq.)\\t\\tValid (freq.)')\n",
        "# we can take a look a the 20 most frequent words + their frequencies:\n",
        "for i in range(20):\n",
        "  print('{0} ({1})\\t\\t{2} ({3})'.format(count_pairs[i][0],count_pairs[i][1],count_pairs_valid[i][0],count_pairs_valid[i][1]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 20 most frequent words:\n",
            "Train (freq.)\t\tValid (freq.)\n",
            "the (50770)\t\tthe (4122)\n",
            "<unk> (45020)\t\t<unk> (3485)\n",
            "<eos> (42068)\t\t<eos> (3370)\n",
            "N (32481)\t\tN (2603)\n",
            "of (24400)\t\tof (1832)\n",
            "to (23638)\t\tto (1750)\n",
            "a (21196)\t\ta (1738)\n",
            "in (18000)\t\tin (1392)\n",
            "and (17474)\t\tand (1391)\n",
            "'s (9784)\t\t's (868)\n",
            "that (8931)\t\tfor (726)\n",
            "for (8927)\t\t$ (659)\n",
            "$ (7541)\t\tthat (657)\n",
            "is (7337)\t\tit (537)\n",
            "it (6112)\t\tis (529)\n",
            "said (6027)\t\tsaid (513)\n",
            "on (5650)\t\ton (486)\n",
            "by (4915)\t\tat (453)\n",
            "at (4894)\t\twas (436)\n",
            "as (4833)\t\tas (402)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wCHz8MhRBvoQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given that the training text is much larger than the validation text, it is normal that the absolute frequencies in the training text are much larger. The ranking of the words is more interesting, and we see that even in the top 20, there are small differences. For the medium- and low-frequency ranges, the differences will become larger:"
      ]
    },
    {
      "metadata": {
        "id": "X0lwxGw9Ap_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "4aeb286c-0dc5-46b1-d9d1-3d5afc8f5d89"
      },
      "cell_type": "code",
      "source": [
        "print('Train (freq.)\\t\\tValid (freq.)')\n",
        "for i in range(200,250):\n",
        "  print('{0} ({1})\\t\\t{2} ({3})'.format(count_pairs[i][0],count_pairs[i][1],count_pairs_valid[i][0],count_pairs_valid[i][1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (freq.)\t\tValid (freq.)\n",
            "well (462)\t\tended (40)\n",
            "part (461)\t\trevenue (40)\n",
            "fell (459)\t\tsee (40)\n",
            "japan (459)\t\tseveral (40)\n",
            "another (457)\t\tdays (39)\n",
            "should (457)\t\tget (39)\n",
            "higher (453)\t\thigher (39)\n",
            "debt (452)\t\tincluding (39)\n",
            "offer (448)\t\tblack (38)\n",
            "take (448)\t\tclose (38)\n",
            "including (445)\t\tfirms (38)\n",
            "among (444)\t\tgeneral (38)\n",
            "court (444)\t\tissues (38)\n",
            "being (443)\t\twell (38)\n",
            "according (442)\t\taround (37)\n",
            "each (442)\t\tchicago (37)\n",
            "index (440)\t\tconcern (37)\n",
            "tax (437)\t\tdrop (37)\n",
            "trade (431)\t\thigh (37)\n",
            "world (431)\t\tmight (37)\n",
            "reported (430)\t\tpoint (37)\n",
            "work (426)\t\tsale (37)\n",
            "operations (424)\t\tsold (37)\n",
            "then (422)\t\tamerican (36)\n",
            "computer (420)\t\tamong (36)\n",
            "past (420)\t\tdecline (36)\n",
            "sale (419)\t\tfinancial (36)\n",
            "however (416)\t\tinternational (36)\n",
            "our (416)\t\tmanagement (36)\n",
            "way (416)\t\tmonday (36)\n",
            "lower (413)\t\tplunge (36)\n",
            "plans (412)\t\tshe (36)\n",
            "vice (412)\t\tsmall (36)\n",
            "economic (410)\t\tagreed (35)\n",
            "department (409)\t\tcapital (35)\n",
            "end (409)\t\tlate (35)\n",
            "yield (409)\t\tlosses (35)\n",
            "report (406)\t\tmade (35)\n",
            "sold (402)\t\tnext (35)\n",
            "insurance (401)\t\topened (35)\n",
            "growth (400)\t\twhere (35)\n",
            "high (400)\t\tboth (34)\n",
            "how (399)\t\tcontract (34)\n",
            "foreign (397)\t\tgovernment (34)\n",
            "increase (397)\t\tlower (34)\n",
            "less (396)\t\tproducts (34)\n",
            "common (395)\t\tset (34)\n",
            "banks (394)\t\tvery (34)\n",
            "closed (394)\t\tworld (34)\n",
            "several (394)\t\tcommon (33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5Fok7dfA6L9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now create a mapping from words to indices. The real input for the neural network will be indices, because they take up less space and because it makes certain operations easier."
      ]
    },
    {
      "metadata": {
        "id": "PGMZqJFVAfoh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# words = list of all the words (in decreasing frequency)\n",
        "items, _ = list(zip(*count_pairs))\n",
        "\n",
        "# make a dictionary with a mapping from each word to an id; word with highest frequency gets lowest id etc.\n",
        "item_to_id = dict(zip(items, range(len(items))))\n",
        "id_to_item = dict(zip(range(len(items)), items))\n",
        "vocab_size = len(item_to_id)\n",
        "\n",
        "# convert the words to indices\n",
        "train_ids_large = [item_to_id[item] for item in train_text]\n",
        "valid_ids_large = [item_to_id[item] for item in valid_text]\n",
        "test_ids_large = [item_to_id[item] for item in test_text]\n",
        "\n",
        "# take a smaller subset to speed up training\n",
        "train_ids = train_ids_large[:50000]\n",
        "valid_ids = valid_ids_large[:10000]\n",
        "test_ids = test_ids_large[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6KQShlZFcGx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the data is converted to ids, it looks like this:"
      ]
    },
    {
      "metadata": {
        "id": "Vlo5SpAHFhLq",
        "colab_type": "code",
        "outputId": "44842da3-b5be-4475-c0b4-fd2863293fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "cell_type": "code",
      "source": [
        "print('Here is an example of words and their indices:')\n",
        "for i in range(40):\n",
        "  print('{0}\\t{1}'.format(valid_text[i], valid_ids[i]))\n",
        "print('\\nAnd this is wat the input looks like, a list of indices:')\n",
        "print(valid_ids[:40])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is an example of words and their indices:\n",
            "consumers\t1132\n",
            "may\t93\n",
            "want\t358\n",
            "to\t5\n",
            "move\t329\n",
            "their\t51\n",
            "telephones\t9836\n",
            "a\t6\n",
            "little\t326\n",
            "closer\t2476\n",
            "to\t5\n",
            "the\t0\n",
            "tv\t662\n",
            "set\t388\n",
            "<eos>\t2\n",
            "<unk>\t1\n",
            "<unk>\t1\n",
            "watching\t2974\n",
            "abc\t2158\n",
            "'s\t9\n",
            "monday\t381\n",
            "night\t1068\n",
            "football\t2347\n",
            "can\t89\n",
            "now\t99\n",
            "vote\t847\n",
            "during\t198\n",
            "<unk>\t1\n",
            "for\t11\n",
            "the\t0\n",
            "greatest\t3383\n",
            "play\t1119\n",
            "in\t7\n",
            "N\t3\n",
            "years\t72\n",
            "from\t20\n",
            "among\t211\n",
            "four\t346\n",
            "or\t36\n",
            "five\t258\n",
            "\n",
            "And this is wat the input looks like, a list of indices:\n",
            "[1132, 93, 358, 5, 329, 51, 9836, 6, 326, 2476, 5, 0, 662, 388, 2, 1, 1, 2974, 2158, 9, 381, 1068, 2347, 89, 99, 847, 198, 1, 11, 0, 3383, 1119, 7, 3, 72, 20, 211, 346, 36, 258]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIhqMJ2CKMxP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building, training and testing neural language models"
      ]
    },
    {
      "metadata": {
        "id": "TuNJPpA-oZRs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now define the classes and functions that we will use for training and testing our language models."
      ]
    },
    {
      "metadata": {
        "id": "jBnJVfPuNzmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The class for an RNN language model is **rnn_lm.rnn_lm()**. We will see later which options we can use."
      ]
    },
    {
      "metadata": {
        "id": "tIHeGqTCW8J-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**batchGenerator.batchGenerator(<dataset\\>)** is class that will generate mini-batches from the data. <dataset\\> is a list of word ids."
      ]
    },
    {
      "metadata": {
        "id": "z_SB0UTRdnU5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "batchGenerator is a class that will iterate over the data set and create **mini-batches** that will be the input for the neural network. A mini-batch contains several sentences/word sequences, and feeding mini-batches instead of a single sentence or a single word to the network speeds up the processing, and also causes better convergence of the model.\n",
        "\n",
        "The batches are matrices of the size **batch_size* x **num_steps**. Batch_size is the number of different sequences in a single batch, and num_steps the length of each  sequence.\n",
        "\n",
        "Here is an example of how batchGenerator can be used. You will notice that the target batch contains the same indices as the input batch, but shifted one (time) step to the right."
      ]
    },
    {
      "metadata": {
        "id": "qHiY_eJ0dpUa",
        "colab_type": "code",
        "outputId": "fc40df09-68b1-4772-8f30-df0d8b4b60c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_steps = 50\n",
        "\n",
        "generator = batchGenerator.batchGenerator(valid_ids, batch_size=batch_size, num_steps=num_steps)\n",
        "input_batch, target_batch, end_reached = generator.generate()\n",
        "print('Shape of the mini-batch: {0}'.format(input_batch.shape))\n",
        "print('This is what an input batch looks like:\\n{0}'.format(input_batch))\n",
        "print('And this is what a target batch looks like:\\n{0}'.format(target_batch))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the mini-batch: (32, 50)\n",
            "This is what an input batch looks like:\n",
            "[[1132   93  358 ...    4  249 1795]\n",
            " [   4    3 3770 ...    2    0  361]\n",
            " [ 967   33   25 ...  769 2737    2]\n",
            " ...\n",
            " [  12    3   48 ... 1470    2   54]\n",
            " [ 505    7    1 ...  660   43  299]\n",
            " [   1 2034    8 ...   11   99   29]]\n",
            "And this is what a target batch looks like:\n",
            "[[  93  358    5 ...  249 1795    1]\n",
            " [   3 3770 1619 ...    0  361    4]\n",
            " [  33   25 2047 ... 2737    2 2158]\n",
            " ...\n",
            " [   3   48    7 ...    2   54 1068]\n",
            " [   7    1   50 ...   43  299 9642]\n",
            " [2034    8  377 ...   99   29   28]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ibvCjKOJI9h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a function which pretty-prints what the mini-batches look like. You can give if a batch as first argument, and the index that you want to look at. In our case, there are 32 sequences in every min-batch, so the indices range between 0 and 31 (in Python, indices always start at 0)."
      ]
    },
    {
      "metadata": {
        "id": "-hMxzS8dHC-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_batch(batch, idx):\n",
        "  for i in range(num_steps):\n",
        "      word = id_to_item[batch[idx][i]]\n",
        "      if word == '<eos>':\n",
        "         print()\n",
        "      else:\n",
        "        print(word, end=' ')\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q0RGWHE1Jnvv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And here are some examples of what the first and fourth sequence of the  input and target batch look like. Try it yourself with some new values."
      ]
    },
    {
      "metadata": {
        "id": "oG3sipRFJh7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "08015235-6fb2-428c-bd47-bcabea0b96d4"
      },
      "cell_type": "code",
      "source": [
        "print_batch(input_batch, 0)\n",
        "print_batch(target_batch, 0)\n",
        "\n",
        "print_batch(input_batch, 3)\n",
        "print_batch(target_batch, 3)\n",
        "\n",
        "# try it yourself:\n",
        "# print_batch(..., ...)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "consumers may want to move their telephones a little closer to the tv set \n",
            "<unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            "two weeks ago viewers of several nbc \n",
            "\n",
            "may want to move their telephones a little closer to the tv set \n",
            "<unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
            "two weeks ago viewers of several nbc <unk> \n",
            "\n",
            "says nbc has been able to charge premium rates for this ad time \n",
            "she would n't say what the premium is but it 's believed to be about N N above regular <unk> rates \n",
            "we were able to get advertisers to use their promotion budget for this because \n",
            "\n",
            "nbc has been able to charge premium rates for this ad time \n",
            "she would n't say what the premium is but it 's believed to be about N N above regular <unk> rates \n",
            "we were able to get advertisers to use their promotion budget for this because they \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CY4KxCsUenOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In run_lm, there are two functions that can be used to train and/or test a model. \n",
        "\n",
        "**run_lm.run_lm():** this function can be called to build, train and test models with different parameter settings. "
      ]
    },
    {
      "metadata": {
        "id": "vBbmu8MyeRjg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**run_lm.run_epoch()**: this is a function that does one pass over the whole dataset. If we are training the model, it will update the parameters and return the perplexity. Otherwise, it will just return the perplexity."
      ]
    },
    {
      "metadata": {
        "id": "UozYDTgTPYBN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "yUR-tGjo7x-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Often the input words for a language model are represented as indices in a vocabulary, or one-hot vectors (where all values are 0 except the index of the word, which has value 1). This representation is a discrete representation, just like in n-gram language models. It has the disadvantage that relationships between words (e.g. the syntactic relationship between 'eat' and 'eating', or the semantic relationship between 'eat' and 'drink') can not be inferred from the word representations. "
      ]
    },
    {
      "metadata": {
        "id": "QNYBzUniRG1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural language models however, do not use this representation as is but first map it to a continuous, lower-dimensional vector, also called *word embedding*. They do this by looking up the index of the word in a weight matrix $\\mathbf{W}$, which is often called the embedding matrix. By training the embedding matrix jointly with the rest of the language model, the resulting word embeddings will have some interesting properties: several syntactic and semantic relationships are encoded as vector offsets in the embedding space. A famous example is the vector offset for male - female, which is shown in the example below:"
      ]
    },
    {
      "metadata": {
        "id": "M3DDEbxR7aka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/lverwimp/RNN_language_modeling/blob/master/kingqueen.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "N2WZsA5RRRYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now train a language model and return the embedding matrix of the trained model:"
      ]
    },
    {
      "metadata": {
        "id": "bQ1ZQsmOBoFZ",
        "colab_type": "code",
        "outputId": "5329851c-a916-478d-d09b-2439378c1f11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "emb_matrix = run_lm(cell='LSTM', \n",
        "                    optimizer='Adam', \n",
        "                    lr=0.01, \n",
        "                    inspect_emb=True, \n",
        "                    train_ids=train_ids, \n",
        "                    valid_ids=valid_ids, \n",
        "                    test_ids=test_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from models/rnn.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path models/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 205.207137658\n",
            "Validation perplexity: 719.099237175\n",
            "Epoch 2\n",
            "Train perplexity: 183.706281768\n",
            "Validation perplexity: 754.872969176\n",
            "Epoch 3\n",
            "Train perplexity: 166.842466179\n",
            "Validation perplexity: 758.458571841\n",
            "Epoch 4\n",
            "Train perplexity: 152.107296048\n",
            "Validation perplexity: 790.319179867\n",
            "Epoch 5\n",
            "Train perplexity: 139.465680327\n",
            "Validation perplexity: 839.929676526\n",
            "('Saved the model to ', 'models/rnn.ckpt')\n",
            "Test perplexity: 624.673774295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tKepamr6R-vX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_closest_words(emb_matrix, word):\n",
        "  if word not in item_to_id:\n",
        "    raise IOError('This item is not in the vocabulary')\n",
        "    \n",
        "  else:\n",
        "    id_w = item_to_id[word]\n",
        "    emb_w = emb_matrix[id_w]\n",
        "    norm_emb_w = emb_w / np.linalg.norm(emb_w)\n",
        "    \n",
        "    top_10 = {}\n",
        "    \n",
        "    # iterate over all words\n",
        "    for idx in range(emb_matrix.shape[0]):\n",
        "      # ignore the word itself\n",
        "      if idx != id_w:\n",
        "        \n",
        "        norm_curr_w = emb_matrix[idx] / np.linalg.norm(emb_matrix[idx])\n",
        "        \n",
        "        cos_sim = np.dot(norm_emb_w, norm_curr_w)\n",
        "        \n",
        "        #cos_sim = np.dot(emb_w, emb_matrix[idx]) / \\\n",
        "        #   norm_emb_w * np.linalg.norm(emb_matrix[idx])\n",
        "        \n",
        "        #print('{0}\\t{1}'.format(id_to_item[idx], cos_sim))\n",
        "        \n",
        "        # keep list of top 10 largest cos similarities\n",
        "        if len(top_10) >= 10:\n",
        "          for sim in top_10.iterkeys():\n",
        "            if cos_sim > sim:\n",
        "              \n",
        "              #print(cos_sim)\n",
        "              #print('add new')\n",
        "              #print(top_10)\n",
        "              \n",
        "              del top_10[sim]\n",
        "              top_10[cos_sim] = id_to_item[idx]\n",
        "              break\n",
        "        \n",
        "        else:\n",
        "          top_10[cos_sim] = id_to_item[idx]\n",
        "          \n",
        "        \n",
        "    print('Words with largest cosine similarity w.r.t. {0}'.format(word))\n",
        "    print('Word\\t\\tCosine similarity')\n",
        "    # sort the top 10 \n",
        "    for sim in sorted(top_10, key=float):\n",
        "      print('{0}\\t\\t{1}'.format(top_10[sim], sim))\n",
        "      \n",
        "     \n",
        "    \n",
        " \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nqy54srADglI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.save('emb_matrix_large.npy', emb_matrix_large)\n",
        "\n",
        "find_closest_words(emb_matrix_large, 'test')\n",
        "find_closest_words(emb_matrix_large, 'cat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0g6ZWggDmqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(emb_matrix_large)\n",
        "print(principalComponents)\n",
        "print(principalComponents[:,0].shape)\n",
        "\n",
        "colors = ['navy', 'turquoise', 'darkorange', 'red', 'black', 'blue','yellow','green']\n",
        "target_names = ['cat', 'dog', 'elephant', 'tiger', 'mouse', 'driving','walking','flying']\n",
        "\n",
        "for color, target_name in zip(colors, target_names):\n",
        "    plt.scatter(principalComponents[item_to_id[target_name], 0], \n",
        "                principalComponents[item_to_id[target_name], 1], \n",
        "                color=color, \n",
        "                label=target_name)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbEp9tdFKXA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training networks"
      ]
    },
    {
      "metadata": {
        "id": "a61RdtDCKhmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training neural networks requires a lot of hyperparameter tuning. The hyperparameters of a neural network are for example the type of cell, its size, the method that is used for updating its parameters (also called 'optimizer' ), the type and strength of regularization, ... . All these hyperparameters have to be chosen before the network can built, trained and tested, and they all have to some extent an influence on the  performance of the model."
      ]
    },
    {
      "metadata": {
        "id": "j3bN2Mvmo_8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks are neural networks that take as input a combination of the standard input and the hidden state of the previous time step. Let's first train a simple recurrent neural network (RNN) as a language model. "
      ]
    },
    {
      "metadata": {
        "id": "zI30kaseLmVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "42u843fb9qq2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now train a simple RNN as language model."
      ]
    },
    {
      "metadata": {
        "id": "KRUa0rDYc7SC",
        "colab_type": "code",
        "outputId": "da37ad5f-526d-434e-b2e2-8f790ae15f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 28579.2624974\n",
            "valid_ppl: 7976.3365824\n",
            "Epoch 2\n",
            "train_ppl: 10458.1135736\n",
            "valid_ppl: 1364.88939657\n",
            "Epoch 3\n",
            "train_ppl: 1536.37432389\n",
            "valid_ppl: 2130.04265576\n",
            "Epoch 4\n",
            "train_ppl: 870.760132806\n",
            "valid_ppl: 709.111377606\n",
            "Epoch 5\n",
            "train_ppl: 713.030860672\n",
            "valid_ppl: 724.993998427\n",
            "test_ppl: 689.391726604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "osTYguHZprBE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You see that both the training perplexity and the validation perplexity decreased during training, which is a good sign. However, notice that the validation perplexity of epoch 5 is slightly higher than the validation perplexity of epoch 4. \n"
      ]
    },
    {
      "metadata": {
        "id": "dWILisHi1xHp",
        "colab_type": "code",
        "outputId": "22aad054-4419-4398-82b7-218e5dfd58df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 8.3480932363e+189\n",
            "valid_ppl: 8.94273109753e+240\n",
            "Epoch 2\n",
            "train_ppl: 8.90212615289e+269\n",
            "valid_ppl: 2.70133700595e+271\n",
            "Epoch 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_ppl: inf\n",
            "valid_ppl: inf\n",
            "Epoch 4\n",
            "train_ppl: inf\n",
            "valid_ppl: inf\n",
            "Epoch 5\n",
            "train_ppl: inf\n",
            "valid_ppl: inf\n",
            "test_ppl: inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AvLOjQy8l_D1",
        "colab_type": "code",
        "outputId": "9f8b12c5-195a-446c-aa6c-34bdff9d450b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "cell_type": "code",
      "source": [
        "emb_matrix_large = run_lm(cell='LSTM', optimizer='Adam', lr=0.01, inspect_emb=True, large_data=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "Train perplexity: 297.18710552\n",
            "Validation perplexity: 234.21438526\n",
            "Epoch 2\n",
            "Train perplexity: 204.754365857\n",
            "Validation perplexity: 213.141694848\n",
            "Epoch 3\n",
            "Train perplexity: 184.758054706\n",
            "Validation perplexity: 204.33167339\n",
            "Epoch 4\n",
            "Train perplexity: 174.919933556\n",
            "Validation perplexity: 200.768920798\n",
            "Epoch 5\n",
            "Train perplexity: 168.839457556\n",
            "Validation perplexity: 199.088644593\n",
            "('Saved the model to ', 'models/rnn.ckpt')\n",
            "Test perplexity: 186.386608675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8wKt7L9aPLfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learning rate"
      ]
    },
    {
      "metadata": {
        "id": "EXDniSW54NfB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Judging from the perplexities above, it seems like the Adam optimizer is a bad choice for training our network! However, the interplay between the different hyperparameters of a neural network is complicated, and it is very well possible that a specific optimizer needs a different learning rate. \n",
        "Let's try a learning rate of 0.01 instead of 1:"
      ]
    },
    {
      "metadata": {
        "id": "pPl3KiSs5kQk",
        "colab_type": "code",
        "outputId": "6c9b9d68-85c8-4af6-f42a-7658ff7e2b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam', lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 1057.75617242\n",
            "valid_ppl: 958.832506035\n",
            "Epoch 2\n",
            "train_ppl: 564.324279113\n",
            "valid_ppl: 826.433342462\n",
            "Epoch 3\n",
            "train_ppl: 433.956799222\n",
            "valid_ppl: 793.489183177\n",
            "Epoch 4\n",
            "train_ppl: 364.952622399\n",
            "valid_ppl: 723.403317321\n",
            "Epoch 5\n",
            "train_ppl: 324.387741371\n",
            "valid_ppl: 699.624224383\n",
            "test_ppl: 532.177735851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gcNjIbzgDUuk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This time, the network is converging nicely. Maybe reducing the learning rate even further helps? Let's try a learning rate of 0.001:"
      ]
    },
    {
      "metadata": {
        "id": "UyUTwGvF7Iyy",
        "colab_type": "code",
        "outputId": "bbccda5f-093f-497f-e27a-1db96cec8ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam', lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 647.90650982\n",
            "valid_ppl: 490.249928851\n",
            "Epoch 2\n",
            "train_ppl: 378.543344185\n",
            "valid_ppl: 413.048300797\n",
            "Epoch 3\n",
            "train_ppl: 300.501997416\n",
            "valid_ppl: 375.886351108\n",
            "Epoch 4\n",
            "train_ppl: 258.467386044\n",
            "valid_ppl: 367.867747945\n",
            "Epoch 5\n",
            "train_ppl: 232.486608834\n",
            "valid_ppl: 356.596671902\n",
            "test_ppl: 286.110701589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nNkyWjQNCjja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see an additional improvement. Let's see what reducing the learning rate even further, to 0.0001, gives:"
      ]
    },
    {
      "metadata": {
        "id": "Zn3TWmSz-SyN",
        "colab_type": "code",
        "outputId": "5299fffd-6aec-4932-c3f2-22bce284dd0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='RNN', optimizer='Adam', lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 1366.65057312\n",
            "valid_ppl: 677.639747264\n",
            "Epoch 2\n",
            "train_ppl: 630.575866312\n",
            "valid_ppl: 690.140848653\n",
            "Epoch 3\n",
            "train_ppl: 625.483174618\n",
            "valid_ppl: 707.694743583\n",
            "Epoch 4\n",
            "train_ppl: 620.683702468\n",
            "valid_ppl: 1629.80400103\n",
            "Epoch 5\n",
            "train_ppl: 668.006540818\n",
            "valid_ppl: 1913.05069198\n",
            "test_ppl: 1789.96267687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6FQejsJzC4TO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we see an interesting result: the training perplexity decreased between epoch 0 and 4, but the validation perplexity is continuously increasing. For epoch 5, even the training perplexity increased again. This is an example of a learning rate that is too small: the steps that the network is making are too small."
      ]
    },
    {
      "metadata": {
        "id": "FdVPv212Lem5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Type of RNN cell"
      ]
    },
    {
      "metadata": {
        "id": "2ZHyiycZCemZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A simple RNN has some disadvantages: it often suffers from the so-called *vanishing and exploding gradients* problem. Neural networks are trained with an algorithm called backpropagation, which computes the gradients of the loss with respect to all parameters in the network. For a language model, the loss of the network is called the *cross entropy*, and it is equal to the average negative log probability for every word in the data. The perplexity of the language model is simply the exponential of the cross entropy. In the case of the simple RNN shown above, the parameters would be the weight matrices $\\mathbf{W}$, $\\mathbf{U}$ and $\\mathbf{V}$ and the bias vectors $\\mathbf{b}$ and $\\mathbf{b_v}$. The gradients of the loss with respect to the parameters $\\mathbf{V}$ and $\\mathbf{b_v}$ can be calculated directly, but the gradients with respect to the other parameters in the network are calculated based on the chain rule, which results in multiplying many terms. Moreover, an RNN is typically *unrolled in time*, which means that you also want to update the weights for the words seen before. If the terms in the multiplication are very small or very  large, they can quickly get even smaller (vanish) or larger (explode). The exploding gradients problem can relatively easily be solved by clipping the (norm of) the gradients if they become too large, the vanishing gradients problem is (at least partially) solved by using another type of RNN cell, such as a long short-term memory (LSTM) cell."
      ]
    },
    {
      "metadata": {
        "id": "L-cN7Ox1_KTT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An LSTM contains two hidden states instead of one, a cell state $\\mathbf{c}_t$ and a hidden state $\\mathbf{h}_t$, and  three gates, the input gate, forget gate and output gate. The gates are shown in the upper part of the figure below: they have a sigmoid activation function, which makes sure that the output values are all between 0 and 1. The forget gate $\\mathbf{f}_t$ is combined with the cell state $\\mathbf{c}_t$: it thus decides which parts of the previous cell state should be forgetten (values close to 0) and which not (values close to 1). A new cell state is then calculated based on a combination of the input gate $\\mathbf{i}_t$, which decides what should be added, and the candidate values $\\mathbf{p}_t$, which are the result of a $tanh$ non-linearity. The new cell state $\\mathbf{c}_t$ is then put through another $tanh$, and combined with the output gate, which decides which part of the input should be let through to the new hidden state $\\mathbf{h}_t$. The last part of the network is the equal to the simple RNN. This [blog post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) gives a great description of how an LSTM cell works."
      ]
    },
    {
      "metadata": {
        "id": "LA-MvI3y-Yd0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/lverwimp/RNN_language_modeling/blob/master/LSTM.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "oQWpcWb9Hu6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We take the optimal combination of optimizer (Adam) and learning rate (0.001) for an RNN, and use it to train an LSTM:"
      ]
    },
    {
      "metadata": {
        "id": "IoKmC2JwCYhq",
        "colab_type": "code",
        "outputId": "0e4e631b-ca0b-420b-dd07-9c7bb54535cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 656.82063114\n",
            "valid_ppl: 529.864473663\n",
            "Epoch 2\n",
            "train_ppl: 424.844078727\n",
            "valid_ppl: 464.263171504\n",
            "Epoch 3\n",
            "train_ppl: 356.302922604\n",
            "valid_ppl: 419.632020537\n",
            "Epoch 4\n",
            "train_ppl: 312.305264384\n",
            "valid_ppl: 397.430353409\n",
            "Epoch 5\n",
            "train_ppl: 279.950894164\n",
            "valid_ppl: 376.370915052\n",
            "test_ppl: 308.514639823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AzvS-x9jIKm3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Surprise! The LSTM gives a worse test perplexity, 308.5, than the RNN with the same hyperparameters, 286.1. Looking at the evolution of the perplexities over epochs, we see that they only slowly decrease. Maybe we need a larger learning rate for an LSTM?"
      ]
    },
    {
      "metadata": {
        "id": "lkIJOt8jHoMn",
        "colab_type": "code",
        "outputId": "79f9de2a-f817-44b7-c302-2141a72bb028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, inspect_emb=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 481.709334113\n",
            "valid_ppl: 396.625967937\n",
            "Epoch 2\n",
            "train_ppl: 286.597902762\n",
            "valid_ppl: 355.204807228\n",
            "Epoch 3\n",
            "train_ppl: 235.102307065\n",
            "valid_ppl: 341.794086337\n",
            "Epoch 4\n",
            "train_ppl: 207.242568642\n",
            "valid_ppl: 341.275447747\n",
            "Epoch 5\n",
            "train_ppl: 188.696659696\n",
            "valid_ppl: 349.23735006\n",
            "test_ppl: 264.915329849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-38546940d49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_lm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspect_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-9679138f901c>\u001b[0m in \u001b[0;36mrun_lm\u001b[0;34m(cell, optimizer, lr, embedding_size, hidden_size, dropout_rate, inspect_emb)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minspect_emb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0minspect_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HSv59sc7JW6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This perplexity is already much better. By further optimizing of the learning rate and/or optimizer, we could probably get even lower perplexities."
      ]
    },
    {
      "metadata": {
        "id": "MGAY_hhfNLj5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Size of the embedding"
      ]
    },
    {
      "metadata": {
        "id": "jZ87PmaPJlib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at the influence of the size of the LSTM on its performance. By default, we train a model with embeddings of size 64 and a hidden layer of size 128. Let's see what happens if we reduce the size of the embedding:"
      ]
    },
    {
      "metadata": {
        "id": "q2d_Kx8zJGk-",
        "colab_type": "code",
        "outputId": "9ad75cc0-6d9a-4337-d957-fbc55eed3f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, embedding_size=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 519.359682418\n",
            "valid_ppl: 5124521.52183\n",
            "Epoch 2\n",
            "train_ppl: 325.127520433\n",
            "valid_ppl: 384.575596662\n",
            "Epoch 3\n",
            "train_ppl: 273.853564489\n",
            "valid_ppl: 366.642785843\n",
            "Epoch 4\n",
            "train_ppl: 245.163913857\n",
            "valid_ppl: 358.26870915\n",
            "Epoch 5\n",
            "train_ppl: 225.340377224\n",
            "valid_ppl: 356.266744414\n",
            "test_ppl: 288.184551098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ranM0ERNIta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Size of the hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "vbkbvBaWLVaj",
        "colab_type": "code",
        "outputId": "84dc1cfe-578c-4200-82f6-e8d81afed187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, hidden_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 522.602473469\n",
            "valid_ppl: 433.018669464\n",
            "Epoch 2\n",
            "train_ppl: 325.534562413\n",
            "valid_ppl: 382.469223656\n",
            "Epoch 3\n",
            "train_ppl: 271.586685795\n",
            "valid_ppl: 365.897337815\n",
            "Epoch 4\n",
            "train_ppl: 241.188401725\n",
            "valid_ppl: 356.627466171\n",
            "Epoch 5\n",
            "train_ppl: 221.587265914\n",
            "valid_ppl: 352.766427407\n",
            "test_ppl: 279.318501236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F8O4aKe2NDBB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Regularization"
      ]
    },
    {
      "metadata": {
        "id": "YFKOAC30M1AA",
        "colab_type": "code",
        "outputId": "1fdacc68-ef0f-4818-af8b-50ee50046552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "run_lm(cell='LSTM', optimizer='Adam', lr=0.01, dropout_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch 1\n",
            "train_ppl: 809.953863498\n",
            "valid_ppl: 658.909363669\n",
            "Epoch 2\n",
            "train_ppl: 584.828653476\n",
            "valid_ppl: 606.689601129\n",
            "Epoch 3\n",
            "train_ppl: 539.409788271\n",
            "valid_ppl: 585.649933925\n",
            "Epoch 4\n",
            "train_ppl: 519.498692319\n",
            "valid_ppl: 574.898769121\n",
            "Epoch 5\n",
            "train_ppl: 508.607510944\n",
            "valid_ppl: 574.182146616\n",
            "test_ppl: 517.1558548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9sYewPcLl3WG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ]
    },
    {
      "metadata": {
        "id": "HQ0wAhRNmATx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now test a trained network by calculating the log probability for a sentence. To do this, we first convert the sentence to indices and then run the model:"
      ]
    },
    {
      "metadata": {
        "id": "TaZ-Wle_C4Pk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_log_prob(cell='LSTM', optimizer='SGD', lr=1, \n",
        "           embedding_size=64, hidden_size=128, \n",
        "           dropout_rate=0.5, train_ids=None,\n",
        "           valid_ids=None, test_sent=None):\n",
        "  \n",
        "  # convert words to indices\n",
        "  test_idx = []\n",
        "  for w in test_sent.split(' '):\n",
        "    if w not in item_to_id:\n",
        "      raise IOError(\"{0} is not part of the vocabulary\".format(w))\n",
        "    else:\n",
        "      test_idx.append(item_to_id[w])\n",
        "\n",
        "  run_lm(cell=cell, \n",
        "         optimizer=optimizer, \n",
        "         lr=lr,\n",
        "         embedding_size=embedding_size,\n",
        "         hidden_size=hidden_size,\n",
        "         dropout_rate=dropout_rate,\n",
        "         inspect_emb=False, \n",
        "         train_ids=train_ids, \n",
        "         valid_ids=valid_ids, \n",
        "         test_ids=test_idx,\n",
        "         test_log_prob=True)\n",
        "\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sp8fOR8mPZ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To get the log probability of a specific sentence, use the following commands:"
      ]
    },
    {
      "metadata": {
        "id": "XMoIfVfSZm1D",
        "colab_type": "code",
        "outputId": "ebe05aa6-1812-4f35-cce9-05461e29a54e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "get_log_prob(test_sent='this is a test')\n",
        "get_log_prob(test_sent='test a a a')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:Saving checkpoint to path models/model.ckpt\n",
            "Log probability: -15.9990825653\n",
            "INFO:tensorflow:Restoring parameters from models/model.ckpt\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path models/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Log probability: -24.7472014427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dvXDxzgFmcQv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should see that the log probability of 'test a a a' is lower than 'this is a test', which makes sense. You can test your own sentences here:"
      ]
    },
    {
      "metadata": {
        "id": "pdmBlzRwmi94",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get_log_prob('your own test sentence')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}